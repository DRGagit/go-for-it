[["index.html", "DRG/AGIT - Go for IT Einleitung Termine der Live-Webinare 2019/2020 Zum Nachschauen", " DRG/AGIT - Go for IT Bettina Baeßler &amp; Daniel Pinto dos Santos 2020-12-22 Einleitung Big Data, Radiomics, Künstliche Intelligenz – diese IT-Themen sind in der radiologischen Community derzeit heiß diskutiert und stehen bei einer Vielzahl aktueller Fortbildungsveranstaltungen und Kongressen auf dem Programm. Zu Recht, denn sie gehen uns alle an – schließlich dürfte von ihnen abhängen, wie unsere Radiologie der Zukunft aussehen wird. Doch was steht eigentlich hinter solchen Begriffen wie „Big Data“ oder „Radiomics“? Wissen wir, wovon wir sprechen, wenn wir diese Begriffe benutzen, und sprechen wir eigentlich alle über dasselbe? Wie muss die Datenbasis von KI-Systemen aussehen, um aussagefähige Ergebnisse für den Patientennutzen zu bringen? Mit welchen Datensätzen und Algorithmen werden Radiologien künftig zu tun haben? Termine der Live-Webinare 2019/2020 25.03.2019 Theorie 01 - Einführungsveranstaltung 15.04.2019 R 02 - Einstieg in R: erste Schritte 17.06.2019 R 03 - Nächste Schritte in R: bunte Bilder und mehr 08.07.2019 Theorie 04 - Grundzüge in Statistik: sicher signifikant 02.09.2019 R 05 - Hands on – Deskriptive Statistik in R 30.09.2019 Theorie 06 - Wer Test sagt kann auch p sagen? Statistik überall 14.10.2019 R 07 - Theoretisch ja, praktisch auch! Tests in R 18.11.2019 R 08 - Diverse Tipps &amp; Tricks für R: Nützliche Pakete und Rmarkdown 09.12.2019 Theorie 09 - Genug gelernt, jetzt sind die Maschinen dran! Grundzüge Machine Learning 13.01.2020 Theorie 10 - Machine Learning: Test und Fehlermetriken 27.01.2020 R 11 - See one, do one, teach one! Machine Learning in R 10.02.2020 Theorie 12 - Wenn Bilder auch Daten sind: Einführung in Radiomics 08.06.2020 Slicer3D 13 - Hands-on Radiomics: Segmentierung und Feature Extraktion 22.06.2020 Theorie 14 - Radiomics: Machine Learning Techniken für Dimensionsreduktion und Featureselektion 06.07.2020 R 15 - Bilddaten, Datenbilder: Radiomics-Analysen in R Mehr Informationen gibt es auf den Webseiten der Akademie Online. Zum Nachschauen Alle Webinare sind auch als Aufzeichnung auf conrad (erfordert DRG-Login) zu finden. "],["einstieg-in-r-erste-schritte.html", "1 Einstieg in R: erste Schritte 1.1 Lernziele 1.2 Installation von R und RStudio 1.3 Die RStudio Oberfläche 1.4 Arithmetische Operatoren 1.5 Logische Operatoren 1.6 Besondere Operatoren 1.7 Variablen 1.8 Funktionen 1.9 Vektoren 1.10 Dataframes 1.11 Daten einlesen 1.12 Ausblick 1.13 Schlussbemerkungen", " 1 Einstieg in R: erste Schritte Aller Anfang ist schwer, doch wohnt auch jedem Anfang ein Zauber inne (frei nach Hermann Hesse). Entsprechend sollte man sich nicht abschrecken lassen von der scheinbaren Komplexität und dem anfangs etwas unintuitiven Arbeiten. R erfordert sicher eine gewisse Umstellung, denn Analysen in anderen Statistik-Softwarepaketen “zusammenzuklicken” wirkt zunächst einfacher und komfortabler. Doch nicht nur für komplexere Auswertungen und Machine Learning, sondern auch für bessere Nachvollziehbarkeit und Reproduzierbarkeit eigener Auswertungen lohnt der Aufwand sich von graphischen Oberflächen zu lösen und ins kalte Wasser der “Programmierung” einzutauchen. Das Wort programmieren impliziert in gewisser Weise, dass eine Reihe von Befehlen aufgeschrieben wird, die dann vom Computer abgearbeitet werden. Genau so funktioniert R, indem bspw. der Befehl mean() die Anweisung repräsentiert, eine Reihe von Zahlen aufzusummieren und durch ihre Anzahl zu teilen. Dabei bietet R im Gegensatz zu anderen Programmiersprachen den Vorteil, dass die Befehle relativ gut für Menschen lesbar sind, und das eine Reihe von hilfreichen Paketen zur Verfügung stehen, die den Programmieraufwand für Anwendungen wie wir sie beispielsweise in der Medizin benötigen könnten relativ in Grenzen hält. Neben R existiert mit Python noch eine andere sehr weit verbreitete Programmiersprache, die sich in ähnlicher Weise nutzen lässt, im Funktionsumfang aber vermutlich sogar noch etwas mächtiger ist. Die Tatsache, dass wir hier R nutzen und präsentieren ist mehr dem Umstand geschuldet, dass wir R einfach selbst besser beherrschen. Wir befinden uns aber sicherlich nicht in schlechter Gesellschaft, denn auch große Unternehmen außerhalb der Medizin nutzen R für alle möglichen Datenanalysen. Companies that use R (Quelle DataFlair) 1.1 Lernziele Als Einstieg in R werden in diesem Kapitel die folgenden Themen besprochen: R und Rstudio installieren R als Taschenrechner benutzen Arithmetische, logische und besondere Operatoren in R verstehen Variablen erzeugen und mit ihnen arbeiten Funktionen in R benutzen und verstehen Datentabellen anlegen und Werte darin referenzieren Daten aus externen Dateien importieren sich einen Überblick über Daten verschaffen zusätzliche Programmbibliotheken laden gezielt nach Hilfe in Bezug auf Probleme in R suchen 1.2 Installation von R und RStudio Die Installation von R und RStudio sollte verhältnismäßig einfach und selbsterklärend sein. Wichtig zu bemerken ist, dass für diesen Kurs beide Programme benötigt werden. R ist die eigentliche Programmiersprache bzw. das Programm, dass unseren Programmcode letztlich ausführt, wohingegen RStudio eine Anwendung ist, die das Programmieren in R erheblich vereinfacht und viele praktische Funktionen anbietet. 1.2.1 Installation R Als erstes solle R installiert werden. Hierzu unter https://cran.r-project.org die passende Installationsdatei herunterladen und ausführen. R website (https://cran.r-project.org) 1.2.2 Installation RStudio Als nächstes kann dann RStudio installiert werden. Hierfür findet man die entsprechenden Installationsdateien unter https://rstudio.com/products/rstudio/. RStudio website (https://rstudio.com/products/rstudio/) Neben dem kostenlosesn RStudio Desktop bietet RStudio auch noch andere Produkte an für deren Nutzung eine Lizenzgebühr erhoben wird. Diese sind aber nur für professionelle Anwender von Interesse und für uns nicht nötig. 1.3 Die RStudio Oberfläche Nachdem alle nötigen Programme installiert sind, kann RStudio gestarten werden. Die Benutzeroberfläche wurde im Webinar (conrad) ausführlich erklärt. RStudio Oberfläche - über den roten Pfeil kann ein neues Projekt erstellt werden. Es ist von Vorteil sich innerhalb von RStudio sogenannte Projekte anzulegen. Diese stellen bspw. sicher, dass Dateiausgaben eines R-Skriptes im richtigen Ordner landen, bzw. Dateien, die eingelesen werden sollen - wenn sie sich im Projektordner befinden - leichter referenziert werden können. Projekte können über den Button links oben in RStudio erstellt werden, alternativ via File -&gt; New Project.... Üblicherweise sollte beim Start bereits ein leeres Skript im Editorabschnitt der Benutzeroberfläche zu sehen sein. Falls nicht sollte über File -&gt; New File -&gt; R Script ein leeres Skript erzeugt werden. 1.4 Arithmetische Operatoren R kann im einfachsten Falle als überdimensionierter Taschenrechner benutzt werden. Hierzu entweder im Editorbereich den entsprechenden Code eingeben und mittels cmd+enter bzw. strg+enter ausführen oder direkt im Konsolenbereich eingeben und mit enter ausführen. # Addition 3+5 ## [1] 8 # Subtraktion 7-2 ## [1] 5 # Multiplikation 3*4 ## [1] 12 # Division 16/2 ## [1] 8 # Potenzen 2**3 ## [1] 8 Achtung: In R gilt der Punkt . als Dezimaltrenner, entsprechend werden Berechnungen in denen ein Komma , als Dezimaltrenner steht mit einem Fehler quittiert. 1.2 * 3.4 # funktioniert ## [1] 4.08 # 1,2 * 3,4 würde nicht funktionieren und einen Fehler ausgeben!! 1.5 Logische Operatoren Logische Operatoren führen bspw. Vergleiche durch, als Ergebnis wird ein sog. Boolscher Wert zurückgegeben, d.h. entweder wahr TRUE oder falsch FALSE. Man kann es sich wie eine Frage vorstellen, die entsprechend mit ja oder nein beantwortet wird. # größer als 2 &gt; 1 # also etwa die Frage: &quot;Ist 2 größer als 1? - Antwort: &quot;Ja!&quot; ## [1] TRUE # kleiner als 2 &lt; 2 ## [1] FALSE # kleiner gleich (entsprechend &gt;= für größer gleich) 2 &lt;= 2 ## [1] TRUE # gleich 5 == 7 ## [1] FALSE # ungleich 5 != 7 ## [1] TRUE 1.6 Besondere Operatoren Es existieren eine Reihe von besonderen Operatoren, die in gewisser Weise einfache Funktionen sind und bestimmte Effekte erzeugen. Der einfachste besondere Operator ist der Doppelpunkt :. Dieser erzeugt eine Zahlenreihe, beginnend bei der linken Zahl und dann in Schritten von 1 zur rechten Zahl gehend. # Reihenoperator 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 # würde die rechte Zahl mit dem nächsten Schritt überschritten, # wird der letzte Schritt bspw. kleiner als die rechte Zahl sein 1.6:10.1 ## [1] 1.6 2.6 3.6 4.6 5.6 6.6 7.6 8.6 9.6 1.7 Variablen Wirklich spannend wird Programmierung aber erst dann, wenn Werte nicht immer wieder ausgeschrieben werden müssen, sondern vor allem, wenn statt einzelner Werte Variablen erzeugt werden, mit denen dann wiederum weitergearbeitet werden kann, unabhängig von deren genauem Wert. So kann eine Abfolge von Befehlen immer und immer wieder genutzt werden, und erzeugt in Abhängigkeit der Variablenwerte entsprechend unterschiedliche Ergebnisse. In R werden Variablen mithilfe des Zuweisungsoperators &lt;- erzeugt bzw. der Wert einer Variablen zugewiesen. Der Name der Variablen kann dabei frei gewählt werden, muss aber mit einem Buchstaben beginnen. # Zuweisungsoperator &#39;&lt;-&#39; weist rechten Wert der links genannten Variablen zu a &lt;- 9.385 wolke &lt;- 5.772 # entsprechend kann dann mit diesen Variablen gerechnet werden a * wolke ## [1] 54.17022 # Ergebnisse können ebenfalls Variablen zugewiesen werden ergebnis &lt;- a * wolke # gibt man lediglich den Variablennamen an und führt diese Codezeile aus, # erhält man den Wert der Variablen zurück ergebnis ## [1] 54.17022 1.8 Funktionen Funktionen wiederum sind kurze Befehle, die im Hintergrund eine Reihe von Operationen durchführen. Eine einfache Funktion wäre bspw. der oben erwähnte Befehl mean(). Funktionen haben für gewöhnlich eine Reihe von Parametern, die innerhalb der Klammern () übergeben werden. Diese können bspw. Werte sein, die von der Funktion verarbeitet werden, oder auch Werte, die die Funktionsweise eines Befehls modifizieren. # Werte können als Parameter an Funktionen übergeben werden # Mittelwert mean(1:10) ## [1] 5.5 # Runden, erster Parameter wird auf die im zweiten Parameter angegebene Stellen gerundet round(2.1271, 2) ## [1] 2.13 # Parameter können auch Variablen sein round(ergebnis, 1) ## [1] 54.2 1.9 Vektoren In R werden mit Vektoren Aneinanderreihungen von Werten bezeichnet. Diese können entweder Zahlen enthalten oder bspw. auch Zeichenketten aus Buchstaben. Wichtig ist bei Vektoren jedoch, dass alle Werte vom gleichen Typ sein müssen. D.h. entweder sind alle Elemente des Vektors Zahlen oder bspw. Zeichenketten (die wiederum sowohl Buchstaben als auch Zahlen enthalten können, allerdings kann mit den Zahlen dann nicht gerechnet werden, weil R sie wie Text behandelt). Vektoren können mit der Funktion c() erschaffen werden. Elemente innerhalb eines Vektors können mit ihrem Index in eckigen Klammern [] direkt angesprochen werden. vektor &lt;- c(2,3,5,7,9,10) # ein Vektor aus Zahlen # Ausgabe des gesamten Vektors vektor ## [1] 2 3 5 7 9 10 # Ausgabe eines Elements innerhalb eines Vektors, der Index # in den eckigen Klammern entspricht der Position im Vektor vektor[3] ## [1] 5 Mit Vektoren kann ebenso gerechnet werden, wie mit normalen Zahlen. R wendet dabei die Rechenoperation auf jedes Element des Vektors an. Ebenso können mithilfe des Zuweisungsoperators &lt;- einzelne Positionen im Vektor überschrieben werden. Und natürlich können auch Vektoren an Funktionen übergeben werden. vektor[5] &lt;- 14 # überschreibt den Wert 9, der bisher an Position 5 stand mit dem Wert 14 vektor ## [1] 2 3 5 7 14 10 vektor * 2 # jedes Element des Vektors wird mit 2 multiplizert ## [1] 4 6 10 14 28 20 # es können auch Vektoren miteinander multipliziert werden, dabei # werden beide Vektoren durchgegangen, d.h. 1. Element mit 1. Element, # 2. Element mit 2. Element, usw. - haben die Vektoren unterschiedliche # Längen wird beim kürzeren Vektor wieder von vorne begonnen vektor * c(1,2,3) ## [1] 2 6 15 7 28 30 # der Vektor kann an Funktionen übergeben werden mean(vektor) ## [1] 6.833333 1.10 Dataframes Aber so richtig Spaß macht R erst, wenn man größere Datenmengen manipulieren kann. Außerdem braucht man ein Konstrukt indem Bezüge von einzelnen Daten klar sind. Beispielsweise macht es keinen Sinn einen Vektor mit DLP-Werten von CT-Untersuchungen zu haben und einen mit CTDI-Werten und einen anderen mit Untersuchungsbeschreibungen, wenn nicht klar ist, welches CTDI zu welchem DLP gehört. Hierzu bietet R ein Objekt an, das im Weiteren für die allermeisten Anwendungsfälle der Dreh- und Angelpunkt sein wird: das Dataframe. Dataframes kann man sich im Grunde wie Excel-Tabellen vorstellen. Es gibt Zeilen und Spalten, wobei dem Konzept der “tidy data” folgend, jede Zeile eine “Beobachtung” sein sollte und jede Spalte die verschiedenen Variablen für die Beobachtungen enthält. Für ein einfaches Beispiel erstellen wir ein Dataframe mit sechs CT-Untersuchungen (= sechs Beobachtungen) und drei Variablen (= Spalten), nämlich Untersuchungs-ID, Untersuchungsbezeichnung und DLP. Hierfür nutzen wir die Funktion data.frame() und übergeben als Parameter die jeweiligen Spalten, wobei die Werte jeder Spalte als Vektor übergeben werden. # da R Zeilenumbrüche nach Klammern und Kommas ignoriert, können wir etwas # besser lesbaren Code erzeugen, indem wir Funktion und Spalten jeweils # in eigene Zeilen schreiben tabelle &lt;- data.frame( id = c(1, 2, 3, 4, 5), unt = c(&quot;thx&quot;, &quot;abd&quot;, &quot;thx&quot;, &quot;thx&quot;, &quot;abd&quot;), dlp = c(200, 350, 210, 220, 340) ) tabelle ## id unt dlp ## 1 1 thx 200 ## 2 2 abd 350 ## 3 3 thx 210 ## 4 4 thx 220 ## 5 5 abd 340 Ähnlich wie bei Vektoren können wir auch einzelne Werte mithilfe von eckigen Klammern [] referenzieren. Dabei müssen aber für Einzelwerte sowohl Zeile als auch Spalte angegeben werden. Fehlt die Angabe für Zeile oder Spalte wird jeweils die gesamte Zeile bzw. Spalte referenziert. Dabei steht innerhalb der eckigen Klammer [] zuerst die Zeile, dann gefolgt von einem Komma , die Spalte. # holt Wert aus zweiter Zeile, dritter Spalte tabelle[2,3] ## [1] 350 # holt gesamte erste Zeile tabelle[1,] ## id unt dlp ## 1 1 thx 200 # gibt gesamte zweite Spalte aus tabelle[,2] ## [1] &quot;thx&quot; &quot;abd&quot; &quot;thx&quot; &quot;thx&quot; &quot;abd&quot; Da Spalten üblicherweise mit Spaltennamen versehen sind, bietet R auch die etwas einfachere Möglichkeit Spalten mit ihrem Namen anzusprechen, indem man das Dollarzeichens $ nutzt. Der Rückgabewert ist hierbei einfach ein Vektor mit allen Werten der entsprechenden Spalte. tabelle$dlp ## [1] 200 350 210 220 340 Natürlich lassen sich entsprechend auch Werte an entsprechenden Positionen mithilfe des Zuweisungsoperators &lt;- überschreiben, und logischerweise kann man auch mit den Werten aus Dataframes rechnen. # überschreibt den Werte der dritten Spalte in der fünften Zeile tabelle[5,3] &lt;- 500 # berechnet Mittelwert der DLPs mean(tabelle$dlp) ## [1] 296 Eine sehr nützliche Funktion um einen ersten Überblick über ein Dataframe zu bekommen ist die Funktion str(), die die Struktur eines Dataframes ausgibt. str(tabelle) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ id : num 1 2 3 4 5 ## $ unt: chr &quot;thx&quot; &quot;abd&quot; &quot;thx&quot; &quot;thx&quot; ... ## $ dlp: num 200 350 210 220 500 1.11 Daten einlesen Noch viel interessanter werden die Möglichkeiten in R, wenn man Daten aus üblichen Formaten einlesen kann und dann entsprechend damit in R weiterarbeiten kann. Es existieren natürlich auch Möglichkeiten Excel-Dateien direkt einzulesen, da das aber manchmal zu unerwarteten Problemen führen kann, lohnt es sich zumeist die Daten in Excel als CSV-Datei zu speichern und diese dann in R einzulesen. Normalerweise werden in CSV-Dateien, wie der Name schon sagt, Werte mit Komma , getrennt. Da wir aber üblicherweise das Komma als Dezimaltrenner nutzen, werden in den Dateien die Werte anders als gewöhnlich mit Semikolons ; getrennt und das Komma als Dezimaltrenner genutzt. Damit R nicht durcheinandergerät, muss diese Information der einlesenden Funktion read.csv() als Parameter mitgeteilt werden. Alternativ steht eine zweite Funktion bereit read.csv2(), die bereits von der Situation Semikolon-getrennter Werte ausgeht und daher diese Parameter nicht mehr benötigt. Für das folgende und einige weitere Beispiele im Verlauf wird die Datei ct_data.csv benötigt (~Download~). Am einfachsten ist es diese Datei herunterzuladen und anschließend im lokalen Projektordner zu speichern. # alternativ ginge auch einfach read.csv2(&quot;ct_data.csv&quot;) daten &lt;- read.csv(&quot;ct_data.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) str(daten) ## &#39;data.frame&#39;: 2441 obs. of 6 variables: ## $ tag : chr &quot;2018-01-01&quot; &quot;2018-01-01&quot; &quot;2018-01-01&quot; &quot;2018-01-01&quot; ... ## $ untersuchung: chr &quot;Traumaspirale&quot; &quot;Abdomen&quot; &quot;Abdomen&quot; &quot;Thorax&quot; ... ## $ kv : int 120 120 120 120 120 120 100 100 120 100 ... ## $ mas : num 250 142 105 82 179 50 189 189 250 189 ... ## $ ctdi.vol : num 16.9 9.7 7.1 5.6 8 3.4 7.7 7.7 16.9 7.7 ... ## $ dlp : num 3270 1255 454 1221 353 ... 1.12 Ausblick Sind Daten erstmal eingelesen, können mit wenigen Befehlen komplexe Analysen gemacht werden und Daten ansprechend visualisiert werden. Vieles davon wird in den folgenden Kapiteln erklärt. Der folgende Code soll daher nur einen kleinen Ausblick liefern und Appetit machen. library(ggplot2) library(ggrepel) ggplot(data = daten, aes(x = ctdi.vol, y = dlp, label = as.character(tag))) + geom_point(aes(color=untersuchung)) + geom_smooth(method=lm, se=FALSE, color = &quot;#CCCCCC&quot;, linetype = &quot;dashed&quot;) + geom_text_repel(data = subset(daten, ctdi.vol &gt; 35), nudge_x = 15, direction = &quot;y&quot;) + geom_text_repel(data = subset(daten, dlp &gt; 5370), nudge_y = 1000, direction = &quot;x&quot;) + scale_x_continuous(limits = c(0, 45)) + scale_y_continuous(limits = c(0, 6300)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + labs(title = &quot;Punktwolke für DLP in Abhängigkeit von CTDI&quot;, x = &quot;DLP&quot;, y = &quot;CTDI&quot;, color = &quot;Untersuchungsbezeichnung&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 1.13 Schlussbemerkungen Sicherlich ist man im ersten Moment erschrocken von der Fülle der Möglichkeiten und der gleichzeitigen Abwesenheit eines Leitfadens. Keine Menüs in denen man einfach mal nach der passenden Funktion suchen kann. Aber glücklicherweise ist die R Community sehr aktiv, und man wird selbst für die banalsten Probleme mit einer kurzen Google-Suche fündig. Dabei sollte man allerdings besser direkt auf Englisch suchen, weil dies zweifelsohne die Chance erhöht eine Antwort zu finden. Zum Beispiel: \"Wie erstelle ich ein dataframe?\" bzw. \"How to build a dataframe?\". Sicherlich auch empfehlenswert, um das in diesem Kapitel erlernte weiter zu vertiefen, sei an dieser Stelle das Paket Swirl genannt, dass auf spielerische Art und Weise eine Einführung in R vermittelt. "],["nächste-schritte-in-r-bunte-bilder-und-mehr.html", "2 Nächste Schritte in R: bunte Bilder und mehr 2.1 Lernziele 2.2 Grafiken in Base R 2.3 Pakete installieren und laden 2.4 Grafiken mit ggplot2", " 2 Nächste Schritte in R: bunte Bilder und mehr Das Sprichwort “Ein Bild sagt mehr als tausend Worte” stimmt auch und insbesondere, wenn man sich mit Daten beschäftigt. Häufig ist es wesentlich einfacher, sich mithilfe einer guten Visualisierung einen ersten Überblick über Daten zu verschaffen, als anhand relativ abstrakter Maßzahlen, wie beispielsweise Mittelwert, Median, Quartilen, etc. Noch deutlicher wird es, wenn man beispielsweise grobe Zusammenhänge in Daten finden will. Da wir sehr gewohnt sind, mit unseren Augen Muster zu erkennen, können wir in einer Grafik beispielsweise extrem schnell Gruppen und Ausreißer identifizieren, was bei bloßer Betrachung einer Tabelle schier unmöglich wäre. Beipsiel einer einfachen Punktwolke: Leicht erkennt man eine Gruppe von Untersuchungen mit identischem CTDI und dass 80kV-Untersuchungen zu den DLP-Ausreißern gehören. In vielen Anwendungsfällen stellt daher die Datenvisualisierung einen ersten wichtigen Schritt dar. 2.1 Lernziele In diesem Kapitel werden folgende Themen besprochen: Grafiken mithilfe von R-Basisfunktionen zu erstellen zusätzliche Pakete installieren und laden die Syntax von ggplot2 verstehen und Grafiken damit erstellen Punktwolke Balkendiagramm Histogramm / Verteilungsdichte Boxplot 2.2 Grafiken in Base R Die Grafikfunktionen in Basis-R sind verhältnismäßig schnell erlernbar und einfach, allerdings bieten sie nur einen recht beschränkten Funktionsumfang. Trotzdem lohnt es sich, sich auch mit diesen zu beschäftigen, denn für einfache schnelle Auswertungen eignen sie sich ganz gut. Nachdem wir im vorigen Kapitel gesehen haben, wie man einzelne Spalten eines Dataframes gezielt anspricht, können wir aus unserer Datei ct_data.csv (~Download~) beispielsweise eine einfache Grafik erstellen, die DLP gegen CTDI aufträgt. plot(daten$ctdi.vol, daten$dlp) Die Funktion plot() erwartet als Parameter zwei Vektoren gleicher Länge, die die x- und y-Werte repräsentieren. Es ist also nicht zwingend nötig, die Grafik aus einem Dataframe zu erzeugen. Ähnlich einfach können Boxplots und Balkendiagramme erstellt werden. hist(daten$dlp) boxplot(daten$dlp ~ daten$untersuchung) Zwar können auch mit Basis-R erstellte Grafiken optisch ansprechender gestaltet werden, doch ist die Syntax der Befehle an vielen Stellen etwas unintuitiv, weshalb an dieser Stelle nicht weiter darauf eingegangen werden soll. 2.3 Pakete installieren und laden Glücklicherweise hat man in R die Möglichkeit zusätzliche Pakete zu installieren, die den Funktionsumfang erheblich erweitern. Ein solches Paket (oder besser gesagt gleich eine ganze Sammlung solcher Pakete) ist das Tidyverse. Über die Vor- und Nachteile der Benutzung von Funktionen aus dem Tidyverse wird im Internet in regelmäßigen Abständen hitzig debatiert (z.B. contra und pro). In meiner persönlichen Arbeitsweise ist das Tidyverse eine unverzichtbare Säule und fast immer das Laden desselben die erste Zeile Code. Neben Funktionen zur Datenaufbereitung und -bearbeitung enthält das Tidyverse das äußerst umfangreiche ggplot2-Paket, das bei der Erstellung von Grafiken hilft. Doch von vorne. Zur Installation des tidyverse kann man entweder die graphische Oberfläche von RStudio nutzen und dieses über Tools -&gt; Install Packages... installieren oder den entsprechenden Button im Packages-Pane nutzen. Alternativ kann man auch über die Konsole das Paket installieren. install.packages(&quot;tidyverse&quot;) 2.3.1 Ein kurzer Ausflug ins Tidyverse Bevor wir zu den “bunten Bildern” kommen, vielleicht noch ein kurzer Ausflug ins Tidyverse, um einige Feinheiten dieses Paketes zu erläutern. Alle dazugehörigen Pakete folgen der Philosphie der “tidy data”. Idealerweise sollte man sich diese Philosophie im Umgang mit Daten angewöhnen, da es erheblich zur Klarheit der Daten beiträgt und natürlich auch entsprechend die weiteren Auswertungen in R erleichtert. Um alle Pakete des Tidyverse zu laden, reicht ein einfacher Befehl. # die Meldungen, die hierbei auf der Konsole ausgegeben werden, # können erstmal ignoriert werden library(tidyverse) Darüberhinaus enthält das Tidyverse verschiedene Funktionen, die Funktionen aus Basis-R sehr ähnlich sind, aber einige smarte Vorteile bergen. So existiert bspw. zum Einlesen von Daten die Funktion read_csv2(), die nicht nur im Namen nahezu identisch ist mit der bereits bekannten Funktion read.csv2(). Allerdings hat die tidyverse-Funktion read_csv2() den angenehmen Vorteil, dass sie u.a. versucht zu erkennen, ob in einer Spalte nur Daten eines Typus sind (z.B.: Text, Zahlen oder Datumsangaben) und dann direkt ein Dataframe mit korrekter Formatierung anlegt. # die read_csv2() Funktion gibt Meldungen aus, die erinnern sollen mit welchen # Einstellungen die Daten eingelesen wurden und welche Datentypen erkannt wurden. daten &lt;- read_csv2(&quot;ct_data.csv&quot;) ## ℹ Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use `read_delim()` for more control. ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## tag = col_date(format = &quot;&quot;), ## untersuchung = col_character(), ## kv = col_double(), ## mas = col_double(), ## ctdi.vol = col_double(), ## dlp = col_double() ## ) (Es mag an dieser Stelle viellicht noch nicht ganz klar sein, was damit gemeint ist. Die Vorteile werden aber im Verlauf vielleicht offensichtlich. Ich würde daher empfehlen, immer die Tidyverse-Entsprechungen von Base-R Funktionen zu nutzen). Eine weitere essentielle Funktion Erweiterung, die das Tidyverse mit sich bringt, ist der Übergabeoperator %&gt;%. Dieser erlaubt es, mehrere Befehle in für Menschen leichter lesbarer Weise miteinander zu verketten, anstatt dass kompliziert verschachtelte Funktionen benutzt werden müssen. Vereinfacht kann gesagt werden, dass jeweils das Ergebnis einer Funktion als erster Parameter an die folgende Funktion weitergereicht wird. Am einfachsten lässe es sich an einem banalen Beispiel verdeutlichen: aus einer Reihe Zahlen soll zunächst der Mittelwert gebildet werden, dieser soll anschließend auf drei Stellen nach dem Komma gerundet werden. # in Basis-R ergäbe sich dieser verschachtelte Befehl # es ist nicht leicht zu erkennen, dass die Funktion mean() hier # der erste Parameter für die Funktion round() ist, die 3 am Ende # der zweite round(mean(c(12.3432345, 5.834242453, 9.73389543)), 3) ## [1] 9.304 # gleicher Befehl in Tidyverse-Syntax # es kann einfach von links nach rechts eine Kette von Befehlen # gelesen werden c(12.3432345, 5.834242453, 9.73389543) %&gt;% mean() %&gt;% round(3) ## [1] 9.304 2.4 Grafiken mit ggplot2 Um Grafiken mit dem Paket ggplot2, das auch Teil des Tidyverse ist, zu erstellen muss man zunächst grob die zugrundeliegende “Grammatik der Grafiken” verstehen. Die Idee ist, dass ähnlich wie in einem Satz bspw. Subjekt, Verb und Objekt stehen, Grafiken in der Form Daten, Anordnung der Daten und Aussehen der Grafik beschreiben lassen. Grammatik der Grafiken. Dabei ist zu beachten, dass die Funktion ggplot() das Erstellen von Grafiken startet und als ersten Parameter ein Dataframe erwartet. Anschließend muss innerhalb von ggplot() die ästhetische Eigenschaft definiert werden, also bspw. welche Werte auf die x-Achse und welche auf die y-Achse sollen. Dann können mit + jeweils Objekte und Elemente der Grafik hinzugefügt werden. Zwar sind dann im Einzelfall gelegentlich mehr Befehle nötig als in Basis-R, dafür stehen zahlreiche Möglichkeiten zur Verfügung nahezu jedes Element der Grafik bis ins Detail an die eigenen Wünsche anzupassen. Wir nehemen als erstes Beispiel die Punktwolke von oben. ggplot(daten, aes(x = ctdi.vol, y = dlp)) + geom_point() In ähnlicher Weise können auch das Histogramm und der Boxplot reproduziert werden. Außerdem kann natürlich auch das Dataframe als erster Parameter mithilfe des Verkettungsoperators %&gt;% weitergegeben werden. # für das Histogramm wird keine Angabe zur y-Achse benötigt, # dies wird aus der Häufigkeit der Werte berechnet daten %&gt;% ggplot(aes(x = dlp)) + geom_histogram() daten %&gt;% ggplot(aes(x = untersuchung, y = dlp)) + geom_boxplot() Es können nicht nur ästhetische Eigenschaften im Sinne von x- und y-Zuordnung definiert werden, sondern beispielsweise auch die Farbe eines Elements. Im folgenden Beispiel wollen wir die Punkte in der Punktwolke einmal nach Untersuchungsart einfärben. daten %&gt;% ggplot(aes(x = ctdi.vol, y = dlp, color = untersuchung)) + geom_point() Grafiken können ebenfalls in Variablen gespeichert werden und dann weiter mit + um zusätzliche Elemente ergänzt werden. Im folgenden Beispiel wird die Punktwolke mit den farbigen Punkten in einer Variablen gespeichert. Anschließend wird sie mit besseren Beschriftungen versehen und wiederum gespeichert, zuletzt wird ein anderes Aussehen mithilfe der theme_bw() Funktion auf die Grafik angewendet (neben theme_bw() existieren noch weitere wie theme_minimal(), theme_dark(), usw.). buntes.bild &lt;- daten %&gt;% ggplot(aes(x = ctdi.vol, y = dlp, color = untersuchung)) + geom_point() buntes.bild.beschriftet &lt;- buntes.bild + labs(title = &quot;DLP vs. CTDI&quot;, y = &quot;DLP [mGy*cm]&quot;, x = &quot;CTDI [mGy]&quot;, color = &quot;Untersuchungsbezeichnung&quot;) buntes.bild.beschriftet + theme_bw() "],["deskriptive-statistik-in-r.html", "3 Deskriptive Statistik in R 3.1 Lernziele 3.2 Deskriptive Statistiken mit Basis R 3.3 Deskriptive Statistiken im Tidyverse 3.4 Deskriptive Statistiken mit anderen Paketen 3.5 Mehr bunte Bilder", " 3 Deskriptive Statistik in R Visualisierung ist natürlich schon gut um einen ersten schnellen Überblick über Daten zu bekommen, aber früher oder später geht es ans Eingemachte und es müssen harte und genaue Zahlen her. Einige der wichtigen Zahlen, die unsere Daten beschreiben sind sogenannte deskriptive Statistiken. Darunter fallen so einfache Dinge wie der Mittelwert, aber auch andere Werte wie Standardabweichung, Median, Minimum, Maximum und Quartile oder auch seltener gebrauchte Werte wie der Modus, Kurtosis und Varianz. 3.1 Lernziele In diesem Kapitel werden folgende Themen besprochen: deskriptive Statistiken in R erstellen nach Gruppen aufgeteilte deskriptive Statistiken erstellen „tidyverse“-Funktionen nutzen, um tiefere Einblicke in die Daten zu erhalten Boxplots erstellen und verstehen nach Gruppen getrennte Grafiken erstellen 3.2 Deskriptive Statistiken mit Basis R Erfreulicherweise lassen sich die meisten der Werte für einfache deskriptive Statistiken mit Basis R Funktionen berechnen. Die entsprechenden Funktionen sind nach dem zu berechnenden Wert benannt und im Grunde selbsterklärend. min(daten$dlp) ## [1] 1.2 mean(daten$dlp) ## [1] 1126.547 median(daten$dlp) ## [1] 576 max(daten$dlp) ## [1] 5979.3 Es wäre natürlich insbesondere in großen Tabellen müßig, jede Spalte einzeln aufzurufen. Die Funktion summary() hilft hier und berechnet diese Werte und einige mehr gleich für alle Spalten eines Dataframes. Besonders praktisch hieran: für Spalten, die keine numerischen Werte enthalten, erhält man trotzdem einige praktische quantitative Werte. summary(daten) ## tag untersuchung kv mas ## Min. :2018-01-01 Length:2441 Min. : 12.0 Min. : 2.0 ## 1st Qu.:2018-02-24 Class :character 1st Qu.:120.0 1st Qu.: 92.0 ## Median :2018-04-13 Mode :character Median :120.0 Median : 150.0 ## Mean :2018-04-14 Mean :114.5 Mean : 156.9 ## 3rd Qu.:2018-06-04 3rd Qu.:120.0 3rd Qu.: 206.0 ## Max. :2018-07-31 Max. :801.0 Max. :1231.0 ## ctdi.vol dlp ## Min. : 0.05 Min. : 1.2 ## 1st Qu.: 5.50 1st Qu.: 244.6 ## Median : 8.00 Median : 576.0 ## Mean :10.30 Mean :1126.5 ## 3rd Qu.:14.30 3rd Qu.:1795.3 ## Max. :39.60 Max. :5979.3 Mit den zwei Funktionen str(), die uns einen Überblick über die Struktur eines Dataframes gibt, und der summary() Funktion ergibt sich in den meisten Fällen ein recht guter Eindruck der vorliegenden Daten. Aber oftmals ist nicht so sehr der Überblick über die gesamten Daten gefragt, vielmehr wären nach Gruppen getrennte deskriptive Statistiken von Interesse. Hierzu bietet Basis R die Funktion by, die in der Benutzung zwar etwas unintuitiv ist, aber genau diese Funktionsweise abbildet. Als ersten Parameter erwartet by() einen Vektor (meist also eine Spalte eines Dataframes), auf den nach dem als zweiten Parameter übergebenen Gruppenvektor die als dritter Parameter übergebene Funktion angewandt wird. # erster Parameter: die zu untersuchenden Werte # zweiter Parameter: die Gruppenvariable # dritter Parameter: die anzuwendende Funktion (ohne runde Klammern!) by(daten$dlp, daten$untersuchung, median) ## daten$untersuchung: Abdomen ## [1] 714.85 ## ------------------------------------------------------------ ## daten$untersuchung: Becken ## [1] 341.8 ## ------------------------------------------------------------ ## daten$untersuchung: Gesicht ## [1] 379.4 ## ------------------------------------------------------------ ## daten$untersuchung: Kopf ## [1] 1433 ## ------------------------------------------------------------ ## daten$untersuchung: Thorax ## [1] 219 ## ------------------------------------------------------------ ## daten$untersuchung: Traumaspirale ## [1] 2674.9 # die Ausgabe kann bei vielen Gruppen und Funktionen, die lange # Ausgaben produzieren mitunter recht unübersichtlich werden. # Die Ausgabe dieses Befehls wird deshalb hier nicht gezeigt. by(daten, daten$untersuchung, summary) 3.3 Deskriptive Statistiken im Tidyverse Das Berechnen deskriptiver Statistiken ist keine Aufgabe, für die im Tidyverse per se spezielle Funktionen bereitstehen. Trotzdem bieten einige Funktionen umfangreiche Möglichkeiten fast spielerisch mit den Daten zu interagieren und einfach und schnell Daten zu selektieren, zu gruppieren und auszuwerten. library(tidyverse) Die wichtigsten Funktionen sind hierfür die Funktionen select() (wählt Spalten aus), filter() (wählt Zeilen aus), group_by() (bildet Gruppen) und summarise() (führt Berechnungen für Gruppen aus). Mithilfe von arrange() können wir die entstehende Ausgabe auch noch nach einer Spalte sortieren. Diese Funktionen lassen sich in nahezu beliebiger Weise mit %&gt;% verketten und sind trotzdem relativ einfach zu lesen. Für ein erstes Beispiel wollen wir zum Beispiel unsere Daten nach Untersuchungsbeschreibung gruppieren, dann den Median von CTDI (in mGy wie in der ursprünglichen Tabelle und direkt auch durch 10 geteilt in cGy) und DLP berechnen, sowie die Anzahl der jeweiligen Untersuchungen. Und um die Daten noch leichter fassbar zu machen, soll die Ausgabe nach dem Median des CTDI sortiert werden. Der Übersichtlichkeit halber lohnt es sich statt lange Zeilen zu schreiben, die einzelnen Teile der Befehle auf mehrere Zeilen aufzuteilen. daten %&gt;% group_by(untersuchung) %&gt;% summarise( median_ctdi_mGy = median(ctdi.vol), median_ctdi_cGy = median(ctdi.vol) / 10, median_dlp = median(dlp), anzahl = n() ) %&gt;% arrange(-median_ctdi_mGy) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 6 x 5 ## untersuchung median_ctdi_mGy median_ctdi_cGy median_dlp anzahl ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Traumaspirale 16.9 1.69 2675. 273 ## 2 Gesicht 12.2 1.22 379. 97 ## 3 Abdomen 10.0 1.01 715. 682 ## 4 Becken 9.3 0.93 342. 49 ## 5 Kopf 7.7 0.77 1433 523 ## 6 Thorax 4.8 0.48 219 817 In ähnlicher Weise könnten wir auch bspw. zunächst die Daten nach Untersuchungen filtern, bei denen genau 100kV am Gerät eingestellt waren, dann einige der Werte aus dem letzten Beispiel berechnen und zuletzt in der Ausgabe nur diejenigen Untersuchungensbezeichnungen auflisten, die weniger als 100 mal mit 100kV durchgeführt wurden. Zu guter letzt könnte man auch noch die Spalte Anzahl aus der Ausgabe entfernen, sodass nur die berechneten Werte bleiben. daten %&gt;% filter(kv == 100) %&gt;% group_by(untersuchung) %&gt;% summarise( median_ctdi_mGy = median(ctdi.vol), median_dlp = median(dlp), anzahl = n() ) %&gt;% filter(anzahl &lt; 100) %&gt;% select(-anzahl) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 3 ## untersuchung median_ctdi_mGy median_dlp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abdomen 5.95 796. ## 2 Becken 11.2 474. ## 3 Gesicht 12.2 1194. ## 4 Traumaspirale 10.7 2448. Die Stärke des Tidyverse liegt hier sicher in der Möglichkeit, interaktiv Befehle zu verketten. Man schreibt einige Befehle auf, von denen man glaubt, dass sie das gewünschte Ergebnis bringen könnten oder den Weg dorthin darstellen, führt diese aus und betrachtet die Ausgabe. Dann passt man die Befehle an oder erweitert die Kette um weitere bis man schließlich das gewünschte Ergebnis erhält. 3.4 Deskriptive Statistiken mit anderen Paketen Wie bereits erwähnt, ist eine der herausragenden Eigenschaften von R, dass unzählige Pakte existieren, die verschiedene Funktionen bereitstellen und so das Arbeiten vereinfachen. Zwei sehr gute Pakete für deskriptive Statisiken sind psych und summarytools. Diese können, wie Tidyverse im vorherigen Kapitel, über die entsprechenden Funktionen von RStudio oder über die Konsole installiert werden. 3.4.1 Psych Aus dem Paket psych können wir die Funktion describe() benutzen, um eine Vielzahl von Maßzahlen zu berechnen. library(psych) describe(daten) ## vars n mean sd median trimmed mad min max ## tag 1 2441 NaN NA NA NaN NA Inf -Inf ## untersuchung* 2 2441 3.64 1.81 4 3.68 1.48 1.00 6.0 ## kv 3 2441 114.47 18.77 120 117.36 0.00 12.00 801.0 ## mas 4 2441 156.93 92.65 150 150.06 85.99 2.00 1231.0 ## ctdi.vol 5 2441 10.30 6.98 8 9.44 5.93 0.05 39.6 ## dlp 6 2441 1126.55 1142.93 576 960.73 622.10 1.20 5979.3 ## range skew kurtosis se ## tag -Inf NA NA NA ## untersuchung* 5.00 -0.49 -1.30 0.04 ## kv 789.00 19.45 731.66 0.38 ## mas 1229.00 1.79 13.34 1.88 ## ctdi.vol 39.55 1.21 1.40 0.14 ## dlp 5978.10 1.18 0.51 23.13 Die üblichen Quartile erhält man, wenn man der describe()-Funktion einen entsprechenden zusätzlichen Parameter übergibt. Ebenfalls kann man describe() mitteilen, dass nicht-numerische Werte übersprungen werden sollen. describe(daten, quant=c(.25,.75), omit = TRUE) ## vars n mean sd median trimmed mad min max range ## kv 3 2441 114.47 18.77 120 117.36 0.00 12.00 801.0 789.00 ## mas 4 2441 156.93 92.65 150 150.06 85.99 2.00 1231.0 1229.00 ## ctdi.vol 5 2441 10.30 6.98 8 9.44 5.93 0.05 39.6 39.55 ## dlp 6 2441 1126.55 1142.93 576 960.73 622.10 1.20 5979.3 5978.10 ## skew kurtosis se Q0.25 Q0.75 ## kv 19.45 731.66 0.38 120.0 120.0 ## mas 1.79 13.34 1.88 92.0 206.0 ## ctdi.vol 1.21 1.40 0.14 5.5 14.3 ## dlp 1.18 0.51 23.13 244.6 1795.3 Eine ebenfalls sehr praktische Funktion ist die der Base-R Funktion by() (s.o.) nachempfundenen describe.by() Funktion. Dieser können die selben Parameter mitgegeben werden, wie der einfachen describe() Funktion. # Da auch hier die Ausgabe sehr lang würde, wird sie hier nicht abgebildet. describe.by(daten, daten$untersuchung, quant=c(.25,.75), omit = TRUE) 3.4.2 Summarytools Auch im Paket summarytools existieren Funktionen, die ähnliche Ausgaben erzeugen wie die Funktion describe() aus dem psych Paket. Welche Pakete man letztenendes benutzen will, ist Frage des persönlichen Geschmacks. Die entsprechenden Funktionen in summarytools sind dscr() bzw. stby() library(summarytools) descr(daten) ## Descriptive Statistics ## daten ## N: 2441 ## ## ctdi.vol dlp kv mas ## ----------------- ---------- --------- --------- --------- ## Mean 10.30 1126.55 114.47 156.93 ## Std.Dev 6.98 1142.93 18.77 92.65 ## Min 0.05 1.20 12.00 2.00 ## Q1 5.50 244.60 120.00 92.00 ## Median 8.00 576.00 120.00 150.00 ## Q3 14.30 1795.30 120.00 206.00 ## Max 39.60 5979.30 801.00 1231.00 ## MAD 5.93 622.10 0.00 85.99 ## IQR 8.80 1550.70 0.00 114.00 ## CV 0.68 1.01 0.16 0.59 ## Skewness 1.21 1.18 19.45 1.79 ## SE.Skewness 0.05 0.05 0.05 0.05 ## Kurtosis 1.40 0.51 731.66 13.34 ## N.Valid 2441.00 2441.00 2441.00 2441.00 ## Pct.Valid 100.00 100.00 100.00 100.00 # Da auch hier die Ausgabe sehr lang würde, wird sie hier nicht abgebildet. stby(daten, daten$untersuchung, descr) Noch viel praktischer, aber leider nicht nach Gruppen aufgetrennt durchführbar, ist die Funktion dfSummary(), insbesondere wenn man ihre Ausgabe weiterleitet an die Funktion view() aus dem summarytools Paket. Diese produziert dann eine ansprechend formatierte HTML-Datei, in der verschiedene Maßzahlen aufgeführt werden. Um nach Gruppen getrennte Ausgaben zu bekommen, kann man beispielsweise die tidyverse filter() Funktion benutzen, muss dann aber die Befehle für jede Gruppe erneut ausführen. # die Ausgaben dieser Funktionen erscheinen nicht auf der Konsole, # sondern im Viewer-Pane von RStudio. Dort findet man auch einen Button, # um die Datei in einem Internetbrowser zu öffnen. # einfacher Fall, um einen Überblick über alle Daten zu erhalten dfSummary(daten) %&gt;% view() # innerhalb einer tidyverse-Kette um bspw. einen Überblick über # die Traumaspiralen zu erhalten daten %&gt;% filter(untersuchung == &quot;Traumaspirale&quot;) %&gt;% dfSummary() %&gt;% view() 3.5 Mehr bunte Bilder Um deskriptive Statistiken auch grafisch nach Gruppen zu trennen, bietet sich die Funktion facet_wrap() an, die ebenfalls Teil von ggplot2 ist. Im Prinzip kann man sich die Funktionsweise ähnlich wie die by() Funktion vorstelen, wobei die Handhabung noch etwas simpler ist. Man definiert einfach wie gewohnt die Grafik die man mit ggplot() erstellen möchte, und fügt facet_wrap() einfach mit + in die Kette der Grafikfunktionen. Innerhalb von facet_wrap() muss noch etwas umständlich die entsprechende Gruppenvariable in die Funktion vars() eingeschlossen werden, aber die Ergebnisse sind sehr ansprechend. # ähnliche Grafik wie am Ende des letzten Kapitels, aber mit # nach Untersuchung getrennten Ausgaben, damit die Punkte # weniger überlappen. daten %&gt;% ggplot(aes(x = ctdi.vol, y = dlp, color = untersuchung)) + geom_point() + facet_wrap(vars(untersuchung)) + labs( x = &quot;CTDIvol in mGy&quot;, y = &quot;DLP in mGy*cm&quot;, title = &quot;Zusammenhang CTDIvol / DLP&quot;, color = &quot;Untersuchungsart&quot; ) + theme_bw() Natürlich funktioniert facet_wrap() nicht nur mit Punktwolken, sondern letztlich mit beliebigen Grafiken. Da sich für deskriptive Statistiken insbesondere Boxplots anbieten, im nachfolgenden Beispiel eine etwas komplexere Grafik. In dem Fall wird zunächst die Variable kv mithilfe der Funktion mutate() statt als numerische Variable als ordinale Variable definiert. Zur besseren Übersichtlichkeit werden nur die Fälle betrachtet mit 80kV, 100kV und 120kV. Der Rest ist dann wie gewohnt die Definition der Grafik und das Aufteilen mittels facet_wrap(). daten %&gt;% mutate(kv = as.factor(kv)) %&gt;% filter(kv %in% c(80, 100, 120)) %&gt;% ggplot(aes(x = kv, y = dlp, fill = kv)) + geom_boxplot() + facet_wrap(vars(untersuchung)) + labs( x = &quot;kV-Einstellung&quot;, y = &quot;DLP in mGy*cm&quot;, title = &quot;DLP in Abhängigkeit von kV-Einstellung&quot;, fill = &quot;kV-Einstellung&quot; ) + theme_bw() "],["tests-in-r.html", "4 Theoretisch ja, praktisch auch! Tests in R 4.1 Lernziele 4.2 Normalverteilung prüfen 4.3 T-Test und Wilcoxon-Test 4.4 ANOVA", " 4 Theoretisch ja, praktisch auch! Tests in R Üblicherweise geht es nach den deskriptiven Statistiken in den meisten Fällen ans Eingemachte - die statistischen Tests. Nehmen wir an, in zwei Gruppen beobachten wir unterschiedliche Mittelwerte. Eine mögliche Erklärung könnte sein, dass der bloße Zufall hier Unterschiede erscheinen lässt, wo in Wirklichkeit keine sind. Eine andere Möglichkeit wäre dann, dass in der Tat Unterschiede zwischen den Gruppen bestehen, und diese nicht alleine dem Zufall zu schulden sind. Die gesamten theoretischen Grundlagen hier zu wiederholen würde sicher den Rahmen sprengen, insofern beschränken wir uns im Folgenden mit der Durchführung verschiedener statistischer Tests in R. Welcher Test wann der geeignete ist, ist manchmal nicht leicht zu erkennen. Für eine erste grobe Einschätzung kann aber die folgende Tabelle hilfreich sein. Mögliche statistische Tests in Abhängigkeit der Variablen. (Tabelle ursprünglich aus Wikipedia unter Creative Commons Lizenz) 4.1 Lernziele Daten auf Normalverteilung prüfen (Shapiro-Test) Einfache statistische Tests rechnen (T-Test, Wilcoxon-Test) Effektstärke berechnen und eigene Funktionen schreiben ANOVA durchführen 4.2 Normalverteilung prüfen Wie aus obiger Tabelle ersichtlich ist, kann es mitunter zur Auswahl des korrekten statistischen Tests wichtig sein zunächst zu prüfen, ob die Werte einer Variablen normalverteilt sind. Natürlich können wir versuchen visuell abzuschätzen, ob dies der Fall ist. Für ein fiktives Beispiel wären wir zudem interessiert, die Normalverteilung für einen Wert innerhalb verschiedener Gruppen zu untersuchen. Eine schnelle deskriptive Statistik bzw. Visualisierung könnte wie folgt aussehen. Wir nutzen hier die skew()-Funktion aus dem psych Paket, um die Schiefe der Verteilung zu erhalten (bei einer Normalverteilung nahe 0, bei positiven Werten ist der Ausläufer nach rechts länger, bei negativen Werten der Ausläufer nach links). library(psych) daten %&gt;% group_by(untersuchung) %&gt;% summarise(schiefe_dlp = skew(dlp)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 6 x 2 ## untersuchung schiefe_dlp ## &lt;chr&gt; &lt;dbl&gt; ## 1 Abdomen 2.10 ## 2 Becken 3.39 ## 3 Gesicht 2.57 ## 4 Kopf 0.223 ## 5 Thorax 3.58 ## 6 Traumaspirale 0.0290 daten %&gt;% ggplot(aes(x = dlp, fill = untersuchung)) + geom_density() + facet_wrap(vars(untersuchung)) + labs(x = &quot;DLP [mGy*cm]&quot;, y = &quot;&quot;, fill = &quot;Untersuchungsbezeichnung&quot;) + theme_bw() Wir sehen hier bereits, dass die DLP-Werte für Untersuchungen des Kopfes und bei Traumaspiralen vermutlich normalverteilt sind. Doch um wirklich sicher zu gehen, sollte ein entsprechender statistischer Test verwendet werden. Für diese Frage bietet sich der Shapiro-Wilk-Test an. Der Shapiro-Wilk-Test nimmt als sog. Nullhypothese an, dass die Daten normalverteilt sind. Ein kleiner p-Wert zeigt in diesem Fall dann entsprechen an, dass die Alternativhypothese anzunehmen ist, also dass die Daten nicht normalverteilt sind. Erfreulicherweise findet sich für die meisten statistischen Tests in R eine entsprechend benannte Funktion. Um also bspw. die DLP-Werte im gesamten Datensatz auf Normalverteilung zu untersuchen, können wir einfach die Funktion shapiro.test() nutzen. shapiro.test(daten$dlp) ## ## Shapiro-Wilk normality test ## ## data: daten$dlp ## W = 0.8234, p-value &lt; 2.2e-16 In diesem Fall zeigt also der p-Wert &lt; 2,2 x 10-16 an, dass die Alternativhypothese angenommen werden sollte - die DLP-Werte im Gesamtdatensatz also nicht normalverteilt sind. 4.2.1 Ein kurzer Ausflug in Ergebnisobjekte In vielen Fällen sind die Ausgaben solcher statistischen Tests etwas unintuitiv formatiert und die einzelnen Werte weiterzuverwenden erscheint zunächst schwierig. Es lohnt daher an dieser Stelle ein kurzer Ausflug, denn wir können durchaus auf Einzelteile der Ausgabe direkt zugreifen. Um einen etwas tieferen Einblick zu erhalten, speichern wir zunächst das Ergebnis der shapiro.test() Funktion in einer Variablen und schauen uns die Struktur der Variablen an. shapiro.ergebnis &lt;- shapiro.test(daten$dlp) str(shapiro.ergebnis) ## List of 4 ## $ statistic: Named num 0.823 ## ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## $ p.value : num 8.48e-46 ## $ method : chr &quot;Shapiro-Wilk normality test&quot; ## $ data.name: chr &quot;daten$dlp&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; Wie man erkennen kann, handelt es sich um eine Liste von Werten, die einfach—wenn sie nicht in einer Variablen gespeichert sind—gemeinsam auf der Konsole ausgegeben werden. Das Dollarzeichen $ in der Ausgabe der str() Funktion zeigt hier aber bereits an, dass wir auch einzelne Teile des Ergebnisses direkt referenzieren können, ähnlich den Spalten eines Dataframes. shapiro.ergebnis$p.value ## [1] 8.48211e-46 shapiro.ergebnis$p.value &lt; 0.05 ## [1] TRUE Wären wir also bspw. in obigem fiktiven Beispiel daran interessiert die Normalverteilung innerhalb der Subgruppen zu prüfen, könnten wir in Tidyverse-Syntax eine Verkettung von Befehlen schreiben, an deren Ende eine ansprechend formatierte Tabelle stünde, die in einer Spalte auch gleich prüft, ob der p-Wert unterhalb des Signifikanzniveau von 0.05 liegt. daten %&gt;% group_by(untersuchung) %&gt;% summarise(shapiro_p.wert = shapiro.test(dlp)$p.value) %&gt;% mutate(signifikant_0.05 = shapiro_p.wert &lt; 0.05) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 6 x 3 ## untersuchung shapiro_p.wert signifikant_0.05 ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Abdomen 1.48e-30 TRUE ## 2 Becken 4.46e-10 TRUE ## 3 Gesicht 1.01e-12 TRUE ## 4 Kopf 3.06e-20 TRUE ## 5 Thorax 4.29e-41 TRUE ## 6 Traumaspirale 5.19e- 5 TRUE Wir sehen also, dass—entgegen der ursprünglichen Annahme—auch in den Gruppen, die rein visuell und deskriptiv normalverteilt erschienen (Kopf und Traumaspirale), in Wirklichkeit keine Normalverteilung vorliegt. 4.3 T-Test und Wilcoxon-Test Nachdem nun die Daten auf Normalverteilung geprüft wurden, können wir mit dem entsprechenden statistischen Test untersuchen, ob zwischen einzelnen Gruppen ein signifikanter Unterschied in der Verteilung der Werte für eine Variable existiert. In unserem Falle wäre, da die Daten nicht normalverteilt sind, der Wilcoxon-Mann-Whitney-Test die richtige Wahl. Ähnlich wie für den Shapiro-Wilk-Test shapiro.test() existiert in diesem Falle die Funktion wilcoxon.test(). Die Funktion erwartet als Parameter entweder zwei Vektoren mit Zahlenwerten (oder ein Dataframe und eine sogenannte Formel, aber dazu später). Für ein einfaches Beispiel wollen wir den Vergleich der DLP-Werte von Thorax-Untersuchungen mit denen von Abdomen-Untersuchungen untersuchen. Hierzu ziehen wir uns zunächst die entsprechenden Werte aus dem Dataframe heraus und speichern sie in separaten Variablen, anschließend können diese Vektoren der Funktion wilcoxon.test() übergeben werden. Im Übrigen geht R bei T-Test und Wilcoxon-Test davon aus, dass es sich um unverbundene Stichproben handelt. Sicherheitshalber sollte man sich aber angewöhnen, dies auch explizit mithilfe des Parameters paired = FALSE anzugeben. # erst die Funktion pull() extrahiert aus dem Dataframe einen einfachen Vektor dlp.abdomen &lt;- daten %&gt;% filter(untersuchung == &quot;Abdomen&quot;) %&gt;% pull(dlp) dlp.thorax &lt;- daten %&gt;% filter(untersuchung == &quot;Thorax&quot;) %&gt;% pull(dlp) wilcox.test(dlp.abdomen, dlp.thorax, paired = FALSE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: dlp.abdomen and dlp.thorax ## W = 445818, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 Wie erwartet, zeigt hier der p-Wert &lt; 2,2 x 10-16 an, dass die DLP-Werte von Thorax-Untersuchungen und Abdomen-Untersuchungen signifikant verschieden sind. Genau so einfach ließe sich entsprechend auch ein T-Test t.test() rechnen, der zwar in diesem Falle vielleicht nicht optimal wäre, weil wie bereits besprochen keine Normalverteilung der Werte vorliegt, aber das ignorieren wir einmal. t.test(dlp.abdomen, dlp.thorax, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: dlp.abdomen and dlp.thorax ## t = 14.719, df = 1032.2, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 554.7071 725.3559 ## sample estimates: ## mean of x mean of y ## 1037.5276 397.4961 4.3.1 Ein kurzer Ausflug in Formeln Mit Formeln werden in R nicht übliche Formeln in der Art f(x) = mx + b bezeichnet, sondern bei der Definition von statistischen Tests oder Modellen ein Ausdruck, der beschreibt welche Zielvariable von welchen Einflussvariablen abhängen soll. Eine Formel in R besteht immer aus einer Tilde ~ und einem oder mehreren Ausdrücken, die jeweils rechts (RHS = right hand side) bzw. links (LHS = left hand side) der Tilde ~ stehen. Die Zielvariable steht dabei links (LHS) und die möglichen Einflussvariablen rechts (RHS). Wollten wir also beispielsweise ausdrücken, dass wir das DLP in Abhängigkeit der Untersuchungsbeschreibung auf statistische Signifikanz untersuchen wollen würden, könnten wir das folgendermaßen ausdrücken: dlp ~ untersuchung. Wollten wir also in unserem Beispiel die beiden Gruppen Thorax und Abdomen vergleichen, könnten wir den Ausdruck daten %&gt;% filter(untersuchung %in% c(\"Abdomen\", \"Thorax\")) nutzen, um ein Dataframe zu erzeugen, das nur diese beiden Gruppen enthält. Bei dem Aufruf der Funktion t.test() müssten wir dieses Dataframe als Parameter data = übergeben und als ersten Parameter die oben genannte Formel dlp ~ untersuchung. # auch hier hilft eine Schreibweise mit Zeilenumbrüchen nicht den Überblick zu verlieren t.test( dlp ~ untersuchung, data = daten %&gt;% filter(untersuchung %in% c(&quot;Abdomen&quot;, &quot;Thorax&quot;)), paired = FALSE ) ## ## Welch Two Sample t-test ## ## data: dlp by untersuchung ## t = 14.719, df = 1032.2, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 554.7071 725.3559 ## sample estimates: ## mean in group Abdomen mean in group Thorax ## 1037.5276 397.4961 Das mag zunächst etwas kompliziert wirken. Aber solche Schreibweisen haben den Vorteil, dass nicht für jeden Test neue Variablen definiert werden müssen, was bei langen und komplizierten Auswertungen schonmal dazu führen kann, dass man den Überblick verliert. Für einfache statistische Tests ist die Formelschreibweise nicht unbedingt nötig, und beide Herangehensweisen liefern identische Ergebnisse. Im späteren Kapitel zu Machine Learning werden wir allerdings um die Verwendung dieser Formelschreibweise nicht herum kommen. 4.3.2 Ein kurzer Ausflug in Funktionen Im Live-Webinar hatten wir darüber gesprochen, dass bspw. beim T-Test neben dem p-Wert auch die sog. Effektstärke relevant sein kann. Zwar existiert für R beispielsweise das Paket effsize, das Funktionen enthält um die Effektstärke zu berechnen, aber gelegentlich kommt vielleicht man an einen Punkt, wo auf die Schnelle kein Paket zu finden ist, dass genau die Funktion enthält, die man sucht. Für solche Fälle bietet R die Möglichkeit, eigene Funktionen zu definieren, die der Vollständigkeit hier genannt werden soll, obwohl diese spezielle Funktionalität für den Anfang vielleicht etwas komplex ist. Für ein kurzes Beispiel wollen wir einen Spezialfall betrachten, in dem für unverbundene Stichproben gleicher Gruppengröße die Effektstärke anhand von T-Statistik und df-Wert des T-Testes brechnet werden kann. Wir wollen dabei das Ergebnisobjekt der Funktion t.test() nehmen und direkt die Werte für T-Statistik und df extrahiert und sodann die Effektstärke berechnet werden kann. Eine Funktion kann einfach mithilfe der Funktion function() definiert werden, und in einer Variable gespeichert werden, die genutzt werden kann, um die Funktion aufzurufen. Innerhalb der runden Klammern () von function() können Parameter definiert werden, die dann innerhalb der Funktion verfügbar sind. Beim Aufruf kann auch an selbst definierte Formeln mithilfe des Verkettungsoperators %&gt;% ein Parameter übergeben werden. Die Möglichkeiten, die sich hierdurch ergeben sind schier endlos… meine_cohens_d_funktion &lt;- function(ergebnis_t.test) { # Die Funktion unname() ist hier für eine schönere Ausgabe am Ende # der Funktion nötig. Kann aber auch ignoriert werden. t_statistik &lt;- ergebnis_t.test$statistic %&gt;% unname() df_wert &lt;- ergebnis_t.test$parameter %&gt;% unname() # Formel für Cohens d # siehe auch https://www.uccs.edu/lbecker/ 2 * t_statistik / sqrt(df_wert) } t.test( dlp ~ untersuchung, data = daten %&gt;% filter(untersuchung %in% c(&quot;Abdomen&quot;, &quot;Thorax&quot;)), paired = FALSE ) %&gt;% meine_cohens_d_funktion() ## [1] 0.9162841 4.4 ANOVA Was aber wenn wir nun nicht nur zwei Gruppen auf signifikante Unterschiede hin untersuchen wollen, sondern gleich mehrere? Der Gedanke einfach bspw. mehrere T-Tests hintereinander zu machen, läge natürlich nahe. Aber dabei ergäbe sich ein zentrales Problem: Wenn wir für unsere Test ein Signifikanzniveau von 0.05 ansetzen, die Wahrscheinlichkeit keinen Typ I Fehler zu machen also bei 95% liegt, würde sich bei einem Vergleich von nur drei Gruppen schon eine Wahrscheinlichkeit von knapp 14% ergeben mindestens eine Nullhypothese falsch abzulehnen. Für genau solche Fälle eignen sich sogenannte Varianzanalysen (im Englischen auch als ANOVA = analysis of variance bezeichnet). Diese Verfahren testen im Grunde genommen die Nullhypothese, dass die Mittelwerte aller Gruppen gleich sind. In R kann eine solche Varianzanalyse sehr einfach mit der Funktion aov() durchgeführt werden, wobei der p-Wert hier nur ausgegeben wird, wenn man sich das Ergebnisobjekt genauer ansieht. aov(dlp ~ untersuchung, data = daten) %&gt;% summary() ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## untersuchung 5 1.454e+09 290784001 408.5 &lt;2e-16 *** ## Residuals 2435 1.733e+09 711883 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In diesem Fall sehen wir als, dass wir unsere Nullhypothese verwerfen können, d.h. für das DLP gibt es in Abhängigkeit von der Untersuchungsart signifikante Unterschiede der Mittelwerte zwischen den Gruppen. Dieses Ergebnis ist für sich genommen natürlich wenig hilfreich, denn das wirklich interessante wäre nun zu wissen, welche Gruppen sich auf welche Art unterscheiden. Hierfür können sogenannte post-hoc Tests durchgeführt werden, die die Ergebnisse der Varianzanalyse weiter untersuchen und automatisch für das multiple Testen korrigieren. Ein Beispiel für einen solchen post-hoc Test ist der Tukey-Test, für den R die Funktion TukeyHSD() enthält. aov(dlp ~ untersuchung, data = daten) %&gt;% TukeyHSD() ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = dlp ~ untersuchung, data = daten) ## ## $untersuchung ## diff lwr upr p adj ## Becken-Abdomen -548.90104 -904.7938 -193.0082 0.0001650 ## Gesicht-Abdomen -596.16468 -857.2855 -335.0439 0.0000000 ## Kopf-Abdomen 644.24473 504.3822 784.1072 0.0000000 ## Thorax-Abdomen -640.03148 -764.8412 -515.2218 0.0000000 ## Traumaspirale-Abdomen 1787.49991 1615.1627 1959.8371 0.0000000 ## Gesicht-Becken -47.26364 -469.0022 374.4749 0.9995580 ## Kopf-Becken 1193.14576 833.6449 1552.6466 0.0000000 ## Thorax-Becken -91.13045 -445.0468 262.7859 0.9776902 ## Traumaspirale-Becken 2336.40094 1963.0654 2709.7365 0.0000000 ## Kopf-Gesicht 1240.40941 974.3921 1506.4267 0.0000000 ## Thorax-Gesicht -43.86680 -302.2873 214.5537 0.9967232 ## Traumaspirale-Gesicht 2383.66459 2099.2287 2668.1005 0.0000000 ## Thorax-Kopf -1284.27621 -1419.0301 -1149.5223 0.0000000 ## Traumaspirale-Kopf 1143.25518 963.5854 1322.9250 0.0000000 ## Traumaspirale-Thorax 2427.53139 2259.3137 2595.7491 0.0000000 Das Ergebnis zeigt uns nun, dass außer für die Vergleiche Gesicht/Becken, Thorax/Becken und Thorax/Gesicht, jeweils signifikante Unterschiede der DLP-Mittelwerte bestehen (die p-Werte sind in der letzten Spalte zu finden). Zudem wird die mittlere Differenz sowie das 95%-Konfidenzintervall der Unterschiede mit ausgegeben. 4.4.1 Ein zweiter kurzer Ausfulg in Ergebnisobjekte Gerade solche Ausgaben, wie die der TukeyHSD() Funktion sind manchmal etwas unübersichtlich, obwohl sie ja eigentlich sogar eine Tabelle mit ausgeben. Diese Tabelle aber wiederum weiter zu manipulieren, ist mit R Basisfunktionen kaum möglich. Erfreulicherweise gibt es auch hier aus dem Tidyverse einige praktische Helferfunktionen, die in dem Paket broom enthalten sind. Insbesondere die Funktion tidy() hilft Ergebnistabellen statistischer Tests in saubere Dataframes zu überführen, mit denen dann wie gewohnt weitergearbeitet werden kann. library(broom) # zunächst speichern wir das Ergebnis des Tukey-Tests, wobei natürlich auch eine # direkte Verkettung zur tidy()-Funktion, usw. möglich wäre tukey.ergebnis &lt;- aov(dlp ~ untersuchung, data = daten) %&gt;% TukeyHSD() # Tukey Ergebnisse im tidy-Format # (sieht in der Ausgabe hier online nicht so sehr anders aus, # in RStudio ist der Unterschied deutlicher) tukey.ergebnis %&gt;% tidy() ## # A tibble: 15 x 7 ## term contrast null.value estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 untersuc… Becken-Abdomen 0 -549. -905. -193. 1.65e-4 ## 2 untersuc… Gesicht-Abdomen 0 -596. -857. -335. 1.23e-9 ## 3 untersuc… Kopf-Abdomen 0 644. 504. 784. 0. ## 4 untersuc… Thorax-Abdomen 0 -640. -765. -515. 0. ## 5 untersuc… Traumaspirale-… 0 1787. 1615. 1960. 0. ## 6 untersuc… Gesicht-Becken 0 -47.3 -469. 374. 1.00e+0 ## 7 untersuc… Kopf-Becken 0 1193. 834. 1553. 0. ## 8 untersuc… Thorax-Becken 0 -91.1 -445. 263. 9.78e-1 ## 9 untersuc… Traumaspirale-… 0 2336. 1963. 2710. 0. ## 10 untersuc… Kopf-Gesicht 0 1240. 974. 1506. 0. ## 11 untersuc… Thorax-Gesicht 0 -43.9 -302. 215. 9.97e-1 ## 12 untersuc… Traumaspirale-… 0 2384. 2099. 2668. 0. ## 13 untersuc… Thorax-Kopf 0 -1284. -1419. -1150. 0. ## 14 untersuc… Traumaspirale-… 0 1143. 964. 1323. 0. ## 15 untersuc… Traumaspirale-… 0 2428. 2259. 2596. 0. # sind die Tukey Ergebnisse einmal in ein sauberes Dataframe überführt, können wir # alle Tidyverse-Funktionen in gewohnter Weise nutzen # bspw. nur die signifkanten Ergebnisse anzeigen, nach Größe der mittleren Differenz # sortiert, und das ganze dann graphisch aufbereitet tukey.ergebnis %&gt;% tidy() %&gt;% filter(adj.p.value &lt; 0.05) %&gt;% select(contrast, estimate, adj.p.value) %&gt;% ggplot(aes(x = fct_reorder(contrast, estimate), y = estimate, fill = adj.p.value)) + geom_col() + coord_flip() + labs(x = &quot;Vergleich&quot;, y = &quot;Mittlere Differenz DLP&quot;, fill = &quot;p-Wert&quot;) + theme_bw() "],["diverse-tipps-tricks-für-r.html", "5 Diverse Tipps &amp; Tricks für R 5.1 Lernziele 5.2 Nachvollziehbare Analysen und Rmarkdown 5.3 Styleguide 5.4 Cheatsheets", " 5 Diverse Tipps &amp; Tricks für R 5.1 Lernziele Rmarkdown-Dokumente benutzen Styleguide kennenlernen Cheatsheets für R finden und benutzen 5.2 Nachvollziehbare Analysen und Rmarkdown Nachdem wir uns nun schon etwas in die Materie und R eingearbeitet haben, wollen wir an dieser Stelle einmal unterbrechen und noch einige nütliche Tips &amp; Tricks zusammenfassen. Bislang hatten wir uns hauptsächlich im Editor- bzw. im Konsolen-Abschnitt der RStudio-Oberfläche bewegt. Zwar bietet der Editor natürlich die Möglichkeit Code zu schreiben und auszuführen, und die Ausgabe auf der Konsole ist natürlich nicht schlecht, aber an einigen Ecken und besonders in größeren Projekten ist dieser Arbeitsablauf doch manchmal etwas wenig praktisch. Beispielsweise kann es vorkommen, dass auf Zwischenergebnisse zurückgekommen werden muss, die aber in einem längeren Ergebnisverlauf auf der Konsole versteckt sind. Verteilung der Arbeitszeit in “Data Science”-Projekten Insbesondere wenn man dann bedenkt, dass in “Data Science”-Projekten die meiste Zeit auf das Organisieren und Aufräumen von Daten verwendet wird, wäre für diese Schritte ein möglichst einfacher und gut dokumentierter Workflow hilfreich. Ein weiterer und in diesem Zusammenhang nicht weniger wichtiger Punkt ist aber auch das Teilen von Ergebnissen bspw. mit Kollegen, die R und RStudio nicht nutzen - das pure R-Skript hilft da wenig. Glücklicherweise bietet RStudio aber auch hierfür eine gute Lösung, ohne dass man etwas völlig neues lernen müsste. Eine einfache Auswertung im Editor bzw. als einfaches R-Skript Die für die meisten Zwecke eleganteste Lösung dürfte zweifelsohne das Arbeiten mit R Markdown Dokumenten sein. Dabei handelt es sich um eine Mischung aus sogenanntem Markdown-Text (eine sehr einfache Art der Auszeichnung von Textformaten) und R-Skripten, in welchen der R-Code in sogenannten Code-Chunks organisiert wird. Alles außerhalb dieser Code Chunks ist lediglich Text und wird nicht ausgeführt. Diese Bereiche außerhalb der Chunks kann man aber beispielsweise dazu nutzen seine Auswertungen zu dokumentieren, oder auch einfach nur um Notizen zu machen. Ein neues Rmarkdown-Dokument, der Beispielinhalt kann einfach gelöscht werden Ein neues R Markdown-Dokument kann man z.B. über File -&gt; New file -&gt; R Markdown... erstellen. Den daraufhin erscheinenden Dialog, kann man getrost mit OK bestätigen, und erhält dann nahezu leeres R Markdown-Dokument, in dem lediglich ein wenig Beispielcode enthalten ist. Den Beispielcode kann man einfach löschen, sollte dabei aber darauf achten den Kopfteil des Dokumentes, dessen Anfang und Ende mit --- bezeichnet wird, zu belassen. Da die Dokumentation zu R Markdown kaum zu schlagen ist, an dieser Stelle nur die wichtigsten Informationen. Einen neuen Chunk kann man mit Strg + Alt + I bzw. Cmd + Option + I anlegen oder über den kleinen Button überhalb des Dokumentes. Code innerhalb des Chunks kann einfach wie gewohnt mit Strg + Enter bzw. Cmd + Enter ausführen, die Ausgabe erhält man aber für gewöhnlich direkt unterhalb des Chunks, wo sie auch bestehen bleibt, bis in dem Chunk anderer Code ausgeführt wird. Ein Beispiel für eine Auswertung innerhalb eines Rmarkdown-Dokuments. Die Ausgabe erscheint direkt unterhalb des Code-“Chunks”, die Tabellenausgabe ist etwas angenehmer formatiert. Eine weitere sehr praktische Funktion, ist die, dass sich solche Rmarkdown-Dokumente als HTML-Datei exportieren lassen, in denen dann neben dem R-Code auch die entsprechenden Ausgaben enthalten sind. Diese HTML-Dateien, kann man dann bspw. Kollegen schicken, die R nicht installiert haben, so aber trotzdem nachvollziehen können was getan wurde. Über die Schaltfläche “Knit” lässt sich der Prozess starten, der das Rmarkdown-Dokument inkl. der R-Ausgaben in eine HTML-Datei umwandelt. Wichtig hierbei ist zu bedenken, dass dazu das gesamte Rmarkdown-Dokument ausgeführt wird, Variablen oder Funktionen, die in diesem Dokument nicht enthalten sind, aber auch nicht benutzt werden können. Was wie ein Nachteil klingt ist aber für gewöhnlich sogar eher ein Vorteil, weil man so für sich nochmal prüfen kann, ob tatsächlich alle nötigen Schritte in der richtigen Reihenfolge enthalten sind. Das selbe Rmarkdown-Dokument als HTML-Output. 5.3 Styleguide Ein weiterer wichtiger Punkt, um Code gut lesbar und damit einfacher nachvollziehbar zu machen, ist sich an einen guten Programmierstil zu halten. Sicher gibt es dazu verschiedene Ansichten, und was für den einen funktioniert, mag der andere für Unsinn halten. Letztlich ist man natürlich frei in dem wie man seinen Code organisiert und schreibt, aber insbesondere wenn man mit anderen gemeinsam arbeitet, ist es sicher von Vorteil sich auf einen Stil zu einigen. Einen schönen Anhaltspunkt hierfür liefert z.B. der tidyverse style guide, in dem sich zahlreiche Vorschläge finden wie R-Code möglichst einfach lesbar gehalten wird. So wird beispielsweise vorgeschlagen für die Benennung von Variablen nur Kleinbuchstaben zu verwenden und innerhalb von Namen Unterstriche zu verwenden, falls mehrere Wörter Teil des Namens sind. # Gut daten &lt;- read.csv(&quot;ct_data.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) dosis_daten &lt;- read.csv(&quot;ct_data.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) # Schlecht DaTeN &lt;- read.csv(&quot;ct_data.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) meineDosisDaten &lt;- read.csv(&quot;ct_data.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) Trotzdem, die Vielzahl der R-Styleguides im Internet zeigt, dass es vermutlich kein zwingendes Argument für den einen und gegen einen anderen Stil gibt. Solange es funktioniert, und man sich im Code zurechtfindet ist jeder Stil erlaubt. Vermutlich wird jeder über die Zeit seinen eigenen Stil finden, aber für den Start kann ein bisschen Orientierung nicht schaden. 5.4 Cheatsheets Ein weiterer Nachteil code-basierten Arbeitens gegenüber der Benutzung einer Software, die in Menüs per Klick erreichbare Befehle anbietet, ist dass man sich gelegentlich nicht mehr an den richtigen Befehl oder dessen Parameter erinnert, bzw. dass man eine bestimmte Funktion bräuchte, ihren Namen aber nicht kennt. Wie bereits in der Einleitung gesagt, ist das zwar in den meisten Fällen mit einer schnellen Google-Suche zu erledigen, aber es gibt auch noch eine andere hilfreiche Möglichkeit. Die sogenannten Cheatsheets, also quasi Spickzetteln, bieten meist zu einem Paket (bzw. einer library()), eine schnelle und kompakte Informationssammlung an. Dort findet man auf einen Blick bspw. eine Übersicht über die wichtigsten Funktionen und deren Nutzung, und gelegentlich noch weitere nützliche Informationen. Insbesondere zu den Tidyverse-Paketen gibt es exzellente Cheatsheets, die helfen vielleicht auch mal eine neue Funktion zu entdecken und auszuprobieren, von der man noch gar nicht wusste, dass man sie eigentlich braucht! Das dplyr Cheatsheet Ein paar der nützlichsten Cheatsheets sind hier direkt verlinkt: Datentransformation mit dplyr Datenvisualisierung mit ggplot Buchstabenketten manipulieren mit stringr Datums- und Zeitangaben mit lubridate "],["see-one-do-one-teach-one-machine-learning-in-r.html", "6 See one, do one, teach one! Machine Learning in R 6.1 Lernziele 6.2 Das Iris Dataset 6.3 Explorative Datenvisualisierung 6.4 Lineare Regression 6.5 Logistische Regression 6.6 Datenaufbereitung für Machine Learning 6.7 Support Vector Machine 6.8 Weitere Informationen zu Machine Learning in R", " 6 See one, do one, teach one! Machine Learning in R Nachdem wir uns nun bereits in deskriptive Statistiken und statistische Tests eingearbeitet haben, können wir einen Schritt weiter gehen und kommen zu dem Thema, das aktuell mehr denn je in aller Munde ist - Maschinelles Lernen bzw. Machine Learning. In seinen einfacheren Ausprägungen ist maschinelles Lernen nichts anderes als die Erarbeitung eines statistischen Modells, welches dann wiederum auf neue Daten angewandt werden kann, um bspw. Vorhersagen zu treffen. Künstliche Intelligenz, Maschinelles Lernen und Neuronale Netze Heute werden die Begriffe “Künstliche Intelligenz”, “Maschinelles Lernen” und “Neuronale Netzwerke” teils synonym verwandt. Korrekterweise kann man jedoch sagen, dass die neuronalen Netze ledigliche ein Teilgebiet des maschinellen Lernens sind, das wiederum ein Teil von dem bezeichnet was unter künstlicher Intelligenz verstanden wird. Im folgenden werden wir einige einfachere Algorithmen kennenlernen und in R ausprobiern. Wer danach noch Hunger auf mehr Machine Learning Algorithmen und deren Anwendung in R hat, dem sei die Webseite 101 Machine Learning Algorithms ans Herz gelegt. 6.1 Lernziele Verschiedene Machine Learning Algorithmen in R nutzen Daten in Training- und Testdaten unterteilen Trainierte Modelle auf neue Daten anwenden Klassifikationsgüte berechnen 6.2 Das Iris Dataset In vielen Beipsielen wird online auf das sogenannte Iris Dataset Bezug genommen. Der Datensatz hat deshalb einige Berühmtheit erlangt, der Einfachheit halber verwenden wir ihn deshalb auch für dieses Webinar. data(&quot;iris&quot;) str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... In dem Datensatz enthalten sind Beobachtungen zu 150 verschiedenen Schwertlilien enthalten, jede Beobachtung enthält Angaben zu Länge und Breite der Kelch- und Kronblätter sowie zur Zugehörigkeit zu einer von drei Schwertlilienarten. Die übliche Aufgabe ist es dann anhand dieses Datensatzes einen Algorithmus zu trainieren, der aus den Angaben zur Länge und Breite der Kelch- und Kronblätter Vorhersagen über die Artzugehörigkeit trifft. 6.3 Explorative Datenvisualisierung Ein guter Anfang ist meist sich einen visuellen Überblick über die Daten zu verschaffen. Man könnte beispielsweise die Verteilung der Messwerte als gruppierte Punktwolken darstellen. iris %&gt;% pivot_longer(-Species, names_to = &quot;variable&quot;) %&gt;% ggplot(aes(x = Species, y = value, color = Species)) + geom_jitter() + facet_wrap(vars(variable)) Wie man sieht, müsste es möglich sein anhand der Daten die Artzugehörigkeit abzuschätzen. Rein visuell könnte man vermuten, dass sich dafür insbesondere die Länge der Kronblätter (Petal-Length) eignen müsste. 6.4 Lineare Regression Einer der einfachsten Machine Learning Algorithmen, die Lineare Regression eignet sich zwar nicht zur Vorhersage eines kategorialen Variable, wie sie die Artzugehörigkeit ist, sollte aber hier trotzdem nicht unerwähnt bleiben. Üblicherweise versucht eine lineare Regression einen kontinuierlichen Zahlenwert für eine abhängige Variable aus einer oder mehreren unabhängigen Variablen zu berechnen. In unserem Fall könnten wir beispielsweise versuchen die Breite der Kronblätter (Petal.Width) aus den übrigen Variablen abzuschätzen. Hierzu nutzen wir die Funktion lm(). Erster Parameter dieser Funktion ist eine Formel, wie wir sie bereits im Kapitel zu den statistischen Tests benutzt haben (Abschnitt 4.3.1). In Formeln kann der Punkt . genutzt werden, um alle Variablen (bzw. Spalten) außer der links der Tilde ~ angegebenen zu referenzieren. Der zweite Parameter der Funktion ist das Dataframe, das genutzt werden soll. Da es hier keinen Sinn machen würde die Art einzuschließen, nutzen wir innerhalb der lm()-Funktion ein select(), um die Variable Species auszuschließen. fit &lt;- lm(formula = Petal.Width ~ ., data = iris %&gt;% select(-Species)) summary(fit) ## ## Call: ## lm(formula = Petal.Width ~ ., data = iris %&gt;% select(-Species)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.60959 -0.10134 -0.01089 0.09825 0.60685 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.24031 0.17837 -1.347 0.18 ## Sepal.Length -0.20727 0.04751 -4.363 2.41e-05 *** ## Sepal.Width 0.22283 0.04894 4.553 1.10e-05 *** ## Petal.Length 0.52408 0.02449 21.399 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.192 on 146 degrees of freedom ## Multiple R-squared: 0.9379, Adjusted R-squared: 0.9366 ## F-statistic: 734.4 on 3 and 146 DF, p-value: &lt; 2.2e-16 Die Ausgabe ist auf den ersten Blick nicht besonders eingängig, zeigt aber im Wesentlichen schon, dass signifikante Zusammenhänge zwischen Kronblattbreite und allen anderen Variablen existieren. Natürlich können auch die Ergebnisobjekte von Machine Learning Algorithmen für die weitere Nutzung mit Tidyverse-Paketen aufbereitet werden (siehe Abschnitt 4.4.1). library(broom) tidy(fit) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.240 0.178 -1.35 1.80e- 1 ## 2 Sepal.Length -0.207 0.0475 -4.36 2.41e- 5 ## 3 Sepal.Width 0.223 0.0489 4.55 1.10e- 5 ## 4 Petal.Length 0.524 0.0245 21.4 7.33e-47 fit %&gt;% tidy(conf.int = TRUE) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate(p.value.chr = format.pval(p.value, digits = 2)) %&gt;% ggplot(aes(x = estimate, y = fct_reorder(term, estimate))) + geom_point() + geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0.15) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + geom_text(aes(label = paste(&quot;p-value =&quot;, p.value.chr)), nudge_y = 0.2) + scale_x_continuous(limits = c(-0.8, 0.8)) + labs(y = &quot;&quot;, x = &quot;Estimate&quot;, title = &quot;Linear regression coefficients for Petal.Width&quot;) 6.5 Logistische Regression Einer der einfachtsten Machine Learning Algorithmen zur Vorhersage einer binären Entscheidung (wie bspw. benigne vs. maligne) ist die Logistische Regression. Das Vorgehen in R ist hierbei weitestgehend gleich zu dem bei der linearen Regression, nur dass wir in diesem Fall eben als abhängige Variable eine Variable nehmen müssen, die entweder als Character oder als Factor vorliegt und nur zwei Ausprägungen hat. Im Iris-Datensatz könnten wir also beispeilsweise versuchen anhand von Blattlängen und -breiten zwischen den Arten virginica und versicolor zu unterscheiden. Dazu nutzen wir die Funktion glm() und geben dieser als Parameter family = \"binomial mit. # Erstellen einer Teilmenge des Datensatzes mit nur zwei Spezies iris_binominal &lt;- iris %&gt;% filter(Species %in% c(&quot;virginica&quot;, &quot;versicolor&quot;)) %&gt;% # da die Variable Species weiterhin als Factor mit drei Level # angelegt wäre, nutzen wir die droplevels()-Funktion, um # Fehler zu vermeiden. droplevels() # family = &quot;binomial&quot; gibt hier an, dass nur eine binäre Entscheidung zu treffen ist fit &lt;- glm(Species ~ ., data = iris_binominal, family = &quot;binomial&quot;) summary(fit) ## ## Call: ## glm(formula = Species ~ ., family = &quot;binomial&quot;, data = iris_binominal) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.01105 -0.00541 -0.00001 0.00677 1.78065 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -42.638 25.707 -1.659 0.0972 . ## Sepal.Length -2.465 2.394 -1.030 0.3032 ## Sepal.Width -6.681 4.480 -1.491 0.1359 ## Petal.Length 9.429 4.737 1.991 0.0465 * ## Petal.Width 18.286 9.743 1.877 0.0605 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.629 on 99 degrees of freedom ## Residual deviance: 11.899 on 95 degrees of freedom ## AIC: 21.899 ## ## Number of Fisher Scoring iterations: 10 In diesem konkreten Beispiel erscheint es so, dass lediglich die Kronblattlänge (Petal.Length) signifikant mit der Artzugehörigkeit zusammenhängt. In einem “echten” Projekt könnte man jetzt versuchen das Modell zu optimieren und nicht signifikante Faktoren entfernen, für unsere Zwecke arbeiten wir aber jetzt mit diesem (sicher nicht idealen) Modell weiter. Wir können nun das in der Variablen fit gespeicherte Modell mithilfe der Funktion predict() nutzen, und uns die Vorhersagewerte des Modells zu ausgeben. Mit einigen praktischen Befehlen können wir die Vorhersagen auch gleich als neue Spalte dem ursprünglichen Dataframe hinzufügen. # wir nutzen hier die add_column()-Funktion aus dem tidyverse # und die predict()-Funktion um Werte mithilfe eines Modells # zu berechnen iris_binominal_predictions &lt;- iris_binominal %&gt;% add_column(prediction_value = predict(fit)) # ein Blick in die ersten Zeilen der Daten head(iris_binominal_predictions) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species prediction_value ## 1 7.0 3.2 4.7 1.4 versicolor -11.354482 ## 2 6.4 3.2 4.5 1.5 versicolor -9.932613 ## 3 6.9 3.1 4.9 1.5 versicolor -6.725380 ## 4 5.5 2.3 4.0 1.3 versicolor -10.073036 ## 5 6.5 2.8 4.6 1.5 versicolor -6.563842 ## 6 5.7 2.8 4.5 1.3 versicolor -9.191831 Wie man sieht, gibt das Modell der logistischen Regression uns einen numerieschen Wert (in unserem Fall in der Spalte prediction_value) zurück, der in gewisser Weise aber die Zugehörigkeit zu der jeweiligen Klasse wiederspiegelt. Diese Daten könnten wir nun für eine einfache Visualisierung der Klassifikationsgenauigkeit nutzen. iris_binominal_predictions %&gt;% ggplot(aes(y = prediction_value, x = Species, color = Species)) + geom_jitter() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs(x = &quot;Spezies&quot;, y = &quot;Ausgabewert der logistischen Regression&quot;, color = &quot;&quot;) + theme(legend.position = &quot;none&quot;) Wie wir aus der Grafik unschwer erkennen, scheinen positive Werte für die Ausgabe der logistischen Regression eher für Schwertlilien der Art virginica zu sprechen, negative Werte für die Art versicolor. Nur in je einem Fall liegt dieses Modell für beide Arten liegt das Modell falsch. 6.5.1 Ein kurzer Ausflug in ROC-Analysen Ohne zu sehr darauf eingehen zu wollen, an dieser Stelle ein kurzer Exkurs zu ROC-Analysen. In einer ROC-Kurve wird sozusagen jede Kombination von Sensitivität und Spezifität aufgetragen, die sich in Abhängigkeit von der Variation eines Parameters (in unserem Fall also bspw. der Ausgabewert der logistischen Regression) ergibt. Je näher die Fläche unterhalb der ROC-Kurve am Wert 1 ist, desto besser die diagnostische oder prädiktive Performance eines Modells. In R können ROC-Kurven und dazugehörige Berechnungen sehr einfach mit dem Paket pROC vorgenommen werden. Im Folgenden nehmen wir das Dataframe mit den ergänzten Vorhersagewerten und teilen der Funktion roc() als ersten Parameter den wahren Wert (hier also Species) und als zweiten Parameter den vorhersagenden Wert (hier also prediction_value) mit. library(pROC) iris_binominal_predictions %&gt;% roc(Species, prediction_value) ## ## Call: ## roc.data.frame(data = ., response = Species, predictor = prediction_value) ## ## Data: prediction_value in 50 controls (Species versicolor) &lt; 50 cases (Species virginica). ## Area under the curve: 0.9972 Wie wir erwartet hätte funktioniert unser Modell sehr gut und erreicht eine AUC von 0.99. Noch eingängiger wäre nun auch eine entsprechende grafische Darstellung der Kurve. Erfreulicherweise enthält das Paket pROC auch hierfür Funktionen, insbesondere die Funktion ggroc(), die direkt eine in ggplot weiter modifizierbare Grafik erzeugt. iris_binominal_predictions %&gt;% roc(Species, prediction_value) %&gt;% ggroc(color = &quot;steelblue&quot;, size = 1) + geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color=&quot;grey&quot;, linetype=&quot;dashed&quot;) + coord_fixed() Wollen wir diese Grafik dann noch bspw. um weitere wichtige Informationen ergänzen, können wir auch ganz das einfach erreichen. # zunächst speichern wir unsere ROC-Analyse in einer Variablen roc_result &lt;- iris_binominal_predictions %&gt;% roc(Species, prediction_value) # AUC und optimaler threshold werden berechnet und in Variablen gespeichert roc_auc &lt;- auc(roc_result) roc_optimal_point &lt;- coords(roc_result, &quot;best&quot;, transpose = FALSE) # ROC-Kurve mit Anmerkungen roc_result %&gt;% ggroc(color = &quot;steelblue&quot;, size = 1) + geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color=&quot;grey&quot;, linetype=&quot;dashed&quot;) + coord_fixed() + annotate(&quot;text&quot;, x = 0.05, y = 0.1, label = paste(&quot;AUC =&quot;, roc_auc), hjust = 1) + geom_text(data = roc_optimal_point, aes(x = specificity, y = sensitivity, label = paste(&quot;Best operating point =&quot;, round(threshold, 2), &quot;\\nSensitivity:&quot;, round(sensitivity, 2), &quot;\\nSpecificity:&quot;, round(specificity, 2) )), hjust = 0, vjust = 1, nudge_x = 0.02, nudge_y = -0.02, size = 3) + geom_point(aes(x = roc_optimal_point$specificity, y = roc_optimal_point$sensitivity), color = &quot;orange&quot;, shape = 4, size = 4) 6.6 Datenaufbereitung für Machine Learning Für die Anwendung “komplizierterer” Machine Learning Algorithmen sollte man sich an einige wichtige Regel halten. Ohne, dass wir hier alle Details bedenken können, sind zwei wichtige Schritte sicherlich die Normalisierung der Daten und die Aufteilung in Trainings- und Testdaten. Für beide Arbeitsschritte können wir Funktionen aus dem Paket caret benutzen. Für die Normalisierung der Daten legen wir zunächst mit der Funktion preProcess() ein Objekt an, das die relevanten Informationen enthält, dieses wiederum wenden wir dann mit der schon bekannten Funktion predict() an. library(caret) preprocess_object &lt;- preProcess(iris, method=c(&quot;range&quot;)) iris_normalized &lt;- predict(preprocess_object, iris) iris_normalized %&gt;% pivot_longer(-Species, names_to = &quot;variable&quot;) %&gt;% ggplot(aes(x = Species, y = value, color = Species)) + geom_jitter() + facet_wrap(vars(variable)) + labs(y = &quot;normalized value&quot;) Man erkennt, dass nun alle Werte auf einen Bereich zwischen 0 und 1 abgebildet wurden, was bspw. die Vergleichbarkeit (auch visuell, vgl. Abschnitt 6.3) verbessern kann. Im nächsten Schritt wollen wir die Daten aufteilen, sodass wir einen Datensatz erhalten, den wir zum Training unseres Algorithmus nutzen, und einen Datensatz, den wir zum Testen bzw. Validieren unseres Modells nutzen. Dabei sollte gewährleistet werden, dass in beiden Gruppen alle Werte annähernd ähnlich verteilt sind, die Zuordnung aber möglichst trotzdem zufällig stattfindet. Auch hier können wir uns einer Funktion des Pakets caret bedienen, in dem Fall die Funktion createDataPartition(). Dieser Funktion müssen wir als Parameter noch die Spalte mitgeben, die unsere vorherzusagende Variable enthält (in unserem Fall also iris$Species), welcher Prozentsatz der Daten dem Training zugeordnet werden soll (z.B.: 80%) und ob wir ein list-Objekt erhalten wollen (wenn wir nur eine Aufteilung machen wollen, sollte man hier FALSE wählen). Da die Funktion uns nur die Position der Zeilen des Dataframes, die zur Trainingsgruppe gehören sollen, gibt, ist es praktischer diese in einer Variablen zu speichern, die wir dann nutzen, um aus dem ursprünglichen Dataframe die entsprechenden Zeilen zu selektieren bzw. zu entfernen. training_index &lt;- createDataPartition(iris$Species, p = .8, list = FALSE) iris_training &lt;- iris[training_index,] iris_testing &lt;- iris[-training_index,] Um das Ergebnis zu kontrollieren, können wir bspw. die bekannten Funktionen zur Erstellung deskriptiver Statistiken nutzen (bspw. die Funktionen aus dem Paket summarytools, siehe auch Abschnitt 3.4.2). # Da der Output recht lang ist, führen wir hier nur die Befehle auf. library(summarytools) dfSummary(iris_training) dfSummary(iris_testing) Sobald wir unsere Daten entsprechend aufgeteilt haben, können wir anfangen unsere Modelle zu trainieren. 6.7 Support Vector Machine Ein gern und häufig verwendeter, etwas komplexerer Machine Learning Algorithmus zur Klassifikation auch mehrerer Gruppen ist die Support Vector Machine. Der Algorithmus versucht hierbei möglichst gute Trennungen zwischen den Klassen (auch mehreren) in einem sogenannten Vektorraum zu finden. In R gibt es mehrere Pakete, die Support Vector Machines zur Verfügung stellen, ein Beispiel wäre das Paket e1071. Das Vorgehen ist dem bei der logistischen Regression (siehe 6.5) sehr ähnlich. Der Funktion svm() übergeben wir auch in diesem Falle eine Formel und den entsprechenden Datensatz. Hier ist es allerdings wichtig nur den Trainingsdatensatz zu verwenden. library(e1071) fit &lt;- svm(Species ~ ., data = iris_training) summary(fit) ## ## Call: ## svm(formula = Species ~ ., data = iris_training) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## ## Number of Support Vectors: 46 ## ## ( 8 19 19 ) ## ## ## Number of Classes: 3 ## ## Levels: ## setosa versicolor virginica Ein solches Modell erschließt sich entsprechend nicht ganz so intuitiv wie bspw. eine linieare Regression (6.4). Für die meisten unserer Anwendungen muss dies aber kein Nachteil sein, es sollte aber bei solchen Algorithmen in besonderem Maße darauf geachtet werden, dass letztlich nur die Ergebnisse in dem Testdatensatz bedeutend sind. Gute Ergebnisse in den Trainingsdaten können auch lediglich Ausdruck eines sogenannten Overfittings sein und sind nicht immer auf die Testdaten übertragbar. In einem zweiten Schritt wenden wir, wie auch schon bei den Regressionen, daher nun das trainierte Modell auf neue Daten an - in diesem Fall den Testdatensatz, der nicht Teil des Trainingsprozesses war. iris_test &lt;- iris_testing %&gt;% add_column(prediction = predict(fit, iris_testing)) Zur Bewertung der diagnostischen Genauigkeit des Modells sind eine Reihe Maßzahlen üblich. Erfreulicherweise enthält das o.g. Paket caret auch einige praktische Funktionen, um diese einfach berechnen zu lassen. Die Funktion confusionMatrix() benötigt dazu als Parameter eine Kreuztabelle, die wir in R wiederum mithilfe der Funktion table() erstellen können. table(iris_test$Species, iris_test$prediction) %&gt;% confusionMatrix() ## Confusion Matrix and Statistics ## ## ## setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 10 0 ## virginica 0 0 10 ## ## Overall Statistics ## ## Accuracy : 1 ## 95% CI : (0.8843, 1) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : 4.857e-15 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 1.0000 ## Specificity 1.0000 1.0000 1.0000 ## Pos Pred Value 1.0000 1.0000 1.0000 ## Neg Pred Value 1.0000 1.0000 1.0000 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3333 0.3333 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 1.0000 1.0000 1.0000 Die Funktion confusionMatrix() gibt erfreulicherweise auch gleich die Kreuztabelle mit aus. Dabei entsprechen die Zeilen dem “wahren” Label oder Wert und die Spalten der Vorhersage des Algorithmus. Wir sehen also nun, dass sich der Algorithmus nur in zwei Fällen im Testdatensatz vertan hat (beide Male gab der Algorithmus Art versicolor aus, obwohl es sich um Schwertlilien der Art virginica handelte). 6.8 Weitere Informationen zu Machine Learning in R Natürlich können wir an dieser Stelle nicht alle Machine Learning Algorithmen behandeln, die man in R nutzen kann. Eine tolle Zusammenstellung vieler Algorithmen, inkl. Tutorials in R, findet man auf der Webseite DataScienceDojo. Wie man schnell sieht sind die hier verwendeten nur ein kleiner Ausschnitt. Trotzdem, alleine mit diesen vier Paketen hat man viele der typischerweise in der Radiologie angewandten Techniken zur Verfügung: broom enthält etliche praktische Funktionen, um mit Modellen zu interagieren caret bietet Funktionen zum Präprozessieren und Aufteilen von Daten e1071 stellt Funktionen für Support Vector Machines zur Verfügung pROC ermöglicht ROC-Analysen inkl. deren graphische Darstelung "],["bilddaten-datenbilder-radiomics-analysen-in-r.html", "7 Bilddaten, Datenbilder: Radiomics-Analysen in R 7.1 Lernziele 7.2 Daten einlesen 7.3 Interreader Variabilität 7.4 Normalisierung 7.5 Feature Reduction 7.6 Korrelationsanalyse 7.7 Fitten und Testen des Modells 7.8 Das Ende. Vielen Dank!", " 7 Bilddaten, Datenbilder: Radiomics-Analysen in R Im letzten Kapitel wollen wir uns eine etwas vereinfachte und trotzdem realitätsnahe Radiomics-Analyse vornehmen. Dazu benutzen wir eine Datentabelle mit fiktiven Radiomics-Daten. In dem Beispiel wurden 300 Radiomics-Features für 300 Patienten berechnet, deren histologisches Ergebnis entweder “benigne” oder “maligne” war. Zur Analyse der Interrater-Reliabilität hat ein zweiter Reader für 20 Fälle ebenfalls Radiomics Features berechnet. Das ist ein etwas vereinfachtes Beispiel und viele der nötigen Arbeitsschritte, die man schon vor der Datenanalyse in R beachten sollte sind an dieser Stelle nicht genannt. Wer sich auch in diese Aspekte einlesen will, dem sei die folgende Publikation an der auch Bettina Baeßler maßgeblich beteiligt war ans Herz gelegt: Radiomics in medical imaging—“how-to” guide and critical reflection 7.1 Lernziele In diesem Kapitel werden folgende Themen besprochen: Eine beispielhafte Radiomics-Analyse in R nachvollziehen Fig. 3 aus “Radiomics in medical imaging—“how-to” guide and critical reflection\" 7.2 Daten einlesen Wie in jedem Projekt müssen wir zunächst einmal die Daten einlesen. Dafür benötigen wir die Datei sample_radiomics_data.csv (~Download~). Die Datei ist, wie üblich auf Systemen mit deutschen Spracheinstellungen, so formatiert, dass das Semikolon ; als Trennzeichen verwendet wird und das Komma , als Dezimaltrenner. Im Tidyverse gibt es dafür die Funktion read_csv2(), die genau dies als Vorseinstellung bereits berücksichtigt (im Gegensatz zu read_csv(), die das Komma , als Trennzeichen verwendet und den Punkt . als Dezimaltrenner). radiomics_daten &lt;- read_csv2(&quot;sample_radiomics_data.csv&quot;) Für einen ersten Überblick über die Daten eignet sich wie immer gut die Funktion dfSummary() aus den Paket summarytools. Wer lieber in Basis-R bleiben möchte, verwendet bspw. die Funktion str(). # beides wird hier nicht ausgeführt, da der Output zu lang wäre library(summarytools) dfSummary(radiomics_daten) %&gt;% view() str(radiomics_daten) Wichtig ist hier noch zu erwähnen, dass die Funktion read_csv2() für Spalten, die Text enthalten, automatisch den Typ character annimmt. Das ist für fast alle Fälle das gewünschte Ergebnis, da wir aber im weiteren benötigen werden, dass unsere Zielvariable histo den Typ factor hat, korrigieren wir das hier noch. radiomics_daten &lt;- radiomics_daten %&gt;% mutate(histo = factor(histo)) 7.3 Interreader Variabilität Der nächste wichtige Schritt ist das identifizieren reproduzierbarer Features, bzw. genauer gesagt das Eliminieren nicht-reproduzierbarer Features vor der weiteren Datenanalyse (siehe Grafik in 7.1). In unserem Beispiel wollen wir nun zunächst für jedes der 30 Features den “Concordance Correlation Coefficient” berechnen. Dafür können wir die Funktion CCC() aus dem Paket DescTools nutzen. Allerdings wäre es sehr mühsam jeden der 30 Vergleiche von Hand zu schreiben, die Ergebnisse zu speichern und dann die entsprechenden Features auszuwählen und bspw. in einer Variablen zu speichern. Insbesondere wäre so ein Vorgehen unpraktisch, wenn sich an den Daten etwas ändert, vielleicht sogar eine Variable hinzukäme. Und man müsste immer aufpassen, die richtigen Namen in der entsprechenden Variablen zu speichern. Erfreulicherweise können wir uns vieler praktischer Features aus dem Tidyverse bedienen, um dies in einem einzigen Arbeitsschritt zu erledigen, der dann auch direkt die Namen der nicht-reproduzierbaren Features in einer Variablen speichert, unabhängig von der Zahl der zu untersuchenden Variablen. Die beiden entscheidenden Schritte hier sind die Funktion split() aus Basis-R und die Funktion map_df() aus dem Tidyverse. Letztere Funktion ist extrem nützlich in vielen Anwendungsfällen, die Nutzung erschließt sich aber nicht auf den ersten Blick (daher hier noch zwei Links zur Erklärung, aber nicht frustrieren lassen, wenn es sich trotzdem nicht direkt erschließt: Post von Matt Upson, Post von Rebecca Barter). library(DescTools) # Zur einfacheren Nachvollziehbarkeit kann es praktisch sein die einzelnen Schritte separat auszuführen # und das jeweilige Ergebniss anzuschauen features.nonreproducible &lt;- radiomics_daten %&gt;% # zunächst transformieren wir unsere Daten so, dass eine lange Tabelle # entsteht, die statt 33 Spalten nur 5 Spalten hat, dafür aber statt 320 # Zeilen 9600 Zeilen, sodass eine Spalte den Namen des Features enthält, # und eine weitere den Wert des jeweiligen Features pivot_longer(-c(reader, histo, PatID), names_to = &quot;feature&quot;) %&gt;% # als nächstes wollen wir die Tabelle so transformieren, dass je eine Spalte # für reader_01 und eine für reader_02 entsteht, denn für die Funktion CCC # müssen die Daten in diesem Format vorliegen pivot_wider(names_from = reader, values_from = value) %&gt;% # hier entfernen wir alle Zeilen, in denen es keinen Wert für reader_02 # gibt, der ja nur einen Teil der Daten, nämlich nur 20 Fälle ausgewertet # hatte filter(!is.na(reader_02)) %&gt;% # nun Teilen wir mit split() unser Dataframe in viele Dataframes auf, # die jeweils nur ein Feature enthalten, in unserem Beispiel erhalten # wir hier also eine sog. Liste mit 30 Dataframes split(.$feature) %&gt;% # nun nutzen wir map_df() um die Funktion CCC in jedem der 30 Dataframes # anzuwenden, als Ergebnis erhalten wir ein einziges Dataframe mit den # 30 berechneten CCCs, dem fügen wir noch den Namen der Features als Spalte # hinzu. map_df(~ add_column(CCC(.$reader_01, .$reader_02, ci = &quot;z-transform&quot;, conf.level = 0.95, na.rm = TRUE)$rho.c, feature = unique(.$feature), .before = 1)) %&gt;% # als nächstes filtern wir das Dataframe so, dass wir nur die Zeilen der # Features erhalten, deren CCC-Wert unterhalb der von uns gesetzten Grenze # von 0,8 lag. filter(est &lt; 0.8) %&gt;% # als letzten Schritt holen wir uns die Namen der Features aus der # entsprechenden Spalte des Dataframes, nur diese werden dann in der # oben angegebenen Variable gespeichert. pull(feature) features.nonreproducible ## [1] &quot;feature_10&quot; &quot;feature_11&quot; &quot;feature_22&quot; &quot;feature_30&quot; 7.4 Normalisierung Nachdem wir nun also die nicht-reproduzierbaren Features identifiziert haben, können wir uns daran machen die Daten für das folgende Machine Learning vorzubereiten (siehe Grafik in 7.1). Zunächst benötigen wir also ein Dataframe in dem nur die reproduzierbaren Features enthalten sind und nur Daten von reader_01. radiomics_daten.reproducible &lt;- radiomics_daten %&gt;% filter(reader == &quot;reader_01&quot;) %&gt;% select(- all_of(features.nonreproducible)) Anschließend normalisieren wir die Daten, wie in 6.6 besprochen. library(caret) radiomics_daten.reproducible.norm &lt;- radiomics_daten.reproducible %&gt;% preProcess(method = &quot;range&quot;) %&gt;% predict(radiomics_daten.reproducible) Um das Ergebnis der Normalisierung zu überprüfen, stellen wir uns das ganze einmal graphisch dar. Zur besseren Visualisierung stellen wir die Grafiken direkt nebeneinander dar, das Paket cowplot bietet dafür tolle Funktionalitäten. library(cowplot) plot_grid( radiomics_daten %&gt;% filter(reader == &quot;reader_01&quot;) %&gt;% pivot_longer(-c(reader, PatID, histo), names_to = &quot;feature&quot;) %&gt;% ggplot(aes(x = feature, y = value, fill = histo)) + geom_boxplot() + coord_flip() + theme(legend.position=&quot;top&quot;), radiomics_daten.reproducible.norm %&gt;% filter(reader == &quot;reader_01&quot;) %&gt;% pivot_longer(-c(reader, PatID, histo), names_to = &quot;feature&quot;) %&gt;% ggplot(aes(x = feature, y = value, fill = histo)) + geom_boxplot() + coord_flip() + theme(legend.position=&quot;top&quot;) ) Wir sehen also, dass die Werte alle Features erfolgreich auf einen Bereich zwischen 0 und 1 normalisiert wurden. Als nächstes können wir das Dataframe mit den reproduzierbaren und normalisierten Daten in Trainings und Testdaten aufteilen. index.training &lt;- createDataPartition(radiomics_daten.reproducible.norm$histo, p = 0.8, list = FALSE) %&gt;% as.vector() radiomics_daten.reproducible.norm.train &lt;- radiomics_daten.reproducible.norm[index.training, ] radiomics_daten.reproducible.norm.test &lt;- radiomics_daten.reproducible.norm[-index.training, ] 7.5 Feature Reduction Als nächstes können wir uns nun einem der wichtigsten Schritte in unserem Workflow widmen. Der Auswahl der Features, die potentiell relevant für die Unterscheidung der zwei Gruppen (benigne vs. maligne) sein könnten (siehe Grafik in 7.1). Schließlich wäre es ja wenig hilfreich alle Features in ein Modell zu werfen, denn es kommt nicht selten vor, dass eine Vielzahl von Features keinen echten Informationsgehalt besitzen und in beiden Gruppen annähernd ähnlich verteilt sind. Hierzu gibt es eine Vielzahl von Möglichkeiten. Ein oft benutzter Algorithmus in vielen Radiomics-Publikationen ist der sogenannte “least absolute shrinkage ans selection operator (LASSO)” (zu dem es hier auch ein schönes Tutorial gibt - verwendet das Paket glmnet). Wir wollten hier einen anderen verwenden, nämlich den sogenannten Boruta Algorithmus. Hierzu nutzen wir das Paket Boruta, das die Anwendung recht simpel macht. Zunächst laden wir das entsprechende Paket und entfernen alle nicht benötigten Spalten aus unserem Trainings-Dataframe, sodass nur noch unsere Features und die Gruppenvariable (in unserem Fall als histo) übrig bleiben. library(Boruta) radiomics_daten.reproducible.norm.train.boruta &lt;- radiomics_daten.reproducible.norm.train %&gt;% select(-PatID, -reader) Als nächtes wenden wir die Funktion Boruta() an und übergeben auch hier wieder eine Formel als ersten Parameter, die definiert welche Variable durch welche anderen Variablen erklärt werden soll (in diesem Fall also histo ~ .). my.boruta &lt;- Boruta(histo ~ ., data = radiomics_daten.reproducible.norm.train.boruta) my.boruta ## Boruta performed 76 iterations in 2.421193 secs. ## 5 attributes confirmed important: feature_09, feature_26, feature_27, ## feature_28, feature_29; ## 21 attributes confirmed unimportant: feature_01, feature_02, ## feature_03, feature_04, feature_05 and 16 more; Der Boruta-Algorithmus gibt uns nun an welche Features wichtig zur Unterscheidung zwischen den Gruppen zu sein scheinen. Gelegentlich gibt der Algorithmus auch noch an, dass eine oder mehrere Variablen nicht final entschieden werden konnten. Um auch in diesen Fällen eine endgültige Entscheidung zu treffen, können wir das Ergebnis noch mit der Funktion TentativeRoughFix() bearbeiten um ein finales Ergebnis zu bekommen. my.boruta.final &lt;- TentativeRoughFix(my.boruta) ## Warning in TentativeRoughFix(my.boruta): There are no Tentative attributes! ## Returning original object. my.boruta.final ## Boruta performed 76 iterations in 2.421193 secs. ## 5 attributes confirmed important: feature_09, feature_26, feature_27, ## feature_28, feature_29; ## 21 attributes confirmed unimportant: feature_01, feature_02, ## feature_03, feature_04, feature_05 and 16 more; Da der Boruta-Algorithmus die Variablen in zufälliger Reihenfolge bearbeitet, kann es gelegentlich zu leicht unterschiedlichen Ergebnissen kommen. In unserem Fall sollte der Algorithmus aber vier relevante Features identifizieren: feature_26, feature_27, feature_28 und feature_29. Die Relevanz der jeweiligen Features können wir uns auch leicht visualisieren, indem wir aus dem Ergebnisobjekt my.boruta.final das Element ImpHistory untersuchen. my.boruta.final$ImpHistory %&gt;% as_tibble() %&gt;% select(- starts_with(&quot;shadow&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;feature&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = feature, y = value)) + geom_boxplot() + coord_flip() Wir können leicht erkennen, dass feature_26 am wichtigsten zu sein scheint, d.h. die beste Trennschärfe zwischen den Gruppen bietet, die übrigen drei wichtigen Feature sind relativ nah beieinander. Für die weiteren Analysen können wir nun die als wichtig bestätigten Features aus unserem Trainingsdatensatz selektieren. Die relevanten Features finden wir in dem Element finalDecision innerhalb des Ergebnisobjektes my.boruta.final. # es ist etwas komfortabler die unwichtigen Features in einer Variable zu speichern und # diese dann zu nutzen um sie aus dem Trainingsdatensatz zu entfernen, daher hier der Umweg features.nonrelevant &lt;- my.boruta.final$finalDecision %&gt;% enframe() %&gt;% filter(value != &quot;Confirmed&quot;) %&gt;% pull(name) radiomics_daten.reproducible.norm.train.boruta.relevant &lt;- radiomics_daten.reproducible.norm.train.boruta %&gt;% select(- all_of(features.nonrelevant)) An dieser Stelle kann es sich lohnen einmal die als relevant identifizierten Features einmal in Abhängigkeit der jeweiligen Gruppe zu visualisieren (siehe Grafik in 7.1). radiomics_daten.reproducible.norm.train.boruta.relevant %&gt;% pivot_longer(-histo, names_to = &quot;feature&quot;) %&gt;% ggplot(aes(x = feature, y = value, fill = histo)) + geom_boxplot() Wir sehen, dass in der Tat vier Features identifiziert wurden, die offenbar relevante Unterschiede zwischen den beiden Gruppen (benigne vs. maligne) aufweisen. Bevor wir aber nun zum finalen Schritte, dem Training eines Klassifizierungsmodells schreiten, bleibt noch ein wichtiger Zwischenschritt. 7.6 Korrelationsanalyse In der vorangegangenen Grafik erkennen wir, dass feature_26 und feature_27 sich in etwa ähnlich verhalten, d.h. höhere Werte bei benigner Histologie haben. Gleiches in etwa gilt mit umgekehrten Vorzeichen für feature_28 und feature_29, hier haben Fälle mit maligner Histologie höhere Werte. Es wäre also nun zu prüfen, ob die Werte der Features innerhalb der jeweiligen Feature-Paare korrelieren oder nicht. Sollte sich hier eine starke Korrelation ergeben, wäre es sinnvoll in dem finalen nur jeweils eines der Features des jeweiligen Paares einzuschließen, da sich in eng miteinander korrelierten Features kein relevanter Zugewinn an Informationen ergibt (siehe Grafik in 7.1). Hierzu können wir die Funktion cor() aus Basis-R nutzen, um die Korrelationskoeffizienten für alle Kombinationen von Features zu berechnen. Das Ergebnis dieser Funktion ist eine Korrelationsmatrix, die wir mithilfe der Funktion corrplot() aus dem gleichnamigen Paket corrplot einfach visualisieren können. library(corrplot) radiomics_daten.reproducible.norm.train.boruta.relevant %&gt;% select(-histo) %&gt;% cor() %&gt;% corrplot() Die Vermutung, dass die Werte der Features innerhalb der beiden oben genannten Feature-Paare stark korrelieren bestätigen sich. In einem Fall mit derart wenigen Fällen, können wir das geeignetste Feature der jeweiligen “Cluster” händisch selbst wählen. Gemessen an den “Wichtigkeiten”, die wir als Ergebnis des Boruta-Algorithmus erhalten hatten (siehe 7.5), fällt die Entscheidung recht leicht auf feature_26 und feature_28. In Föllen mit mehr relevanten Features können natürlich auch kompliziertere Verfahren zur Auswahl der Features aus den jeweiligen Clustern nötig sein. Diese behandeln wir an dieser Stelle nicht. Im Allgemeinen kann aber grob gesagt werden, dass in ein finales Modell nicht mehr als etwa 10 Features eingehen sollten. 7.7 Fitten und Testen des Modells Nun kommt der interessanteste aller Schritte, der aber letztlich nach der ganzen Vorarbeit vermutlich der einfachste von allen sein dürfte. Da aufgrund der überschaubaren Anzahl an letztlich relevanten Features kein besonders komplexer Machine Learning-Algorithmus nötig sein dürfte, verwenden wir an dieser Stelle eine einfache logistische Regression (siehe 6.5). Der entsprechende Code ist einfach: der Funktion glm() übergeben wir als Parameter die Formel mit unseren beiden wichtigsten Features (feature_26 und feature_28) und der Zielvariable (histo) sowie die normalisierten Trainingsdaten als Grundlage für das Fitting. fitted.model &lt;- glm(histo ~ feature_26 + feature_28, data = radiomics_daten.reproducible.norm.train, family = &quot;binomial&quot;) summary(fitted.model) ## ## Call: ## glm(formula = histo ~ feature_26 + feature_28, family = &quot;binomial&quot;, ## data = radiomics_daten.reproducible.norm.train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.840e-04 -2.100e-08 -2.100e-08 2.100e-08 1.388e-04 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 314.34 65650.50 0.005 0.996 ## feature_26 -805.91 153344.69 -0.005 0.996 ## feature_28 -78.53 49941.79 -0.002 0.999 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3.0553e+02 on 239 degrees of freedom ## Residual deviance: 6.4825e-08 on 237 degrees of freedom ## AIC: 6 ## ## Number of Fisher Scoring iterations: 25 An dieser Stelle sollte man sich nicht über das etwas seltsame Ergebnis des Fitting wundern. In diesen fiktiven Daten war die Trennschärfe der relevanten Features bewusst absolut gesetzt, weshalb sich hier ein perfekt trennendes Modell ergibt. Und das natürlich auch in den Testdaten, was wir uns dann hier auch noch als ROC-Kurve visualisieren können (siehe auch 6.5.1). radiomics_daten.reproducible.norm.test.predict &lt;- radiomics_daten.reproducible.norm.test %&gt;% add_column(prediction_value = predict(fitted.model, radiomics_daten.reproducible.norm.test)) radiomics_daten.reproducible.norm.test.predict %&gt;% roc(histo, prediction_value) %&gt;% ggroc(color = &quot;steelblue&quot;, size = 1) + geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color=&quot;grey&quot;, linetype=&quot;dashed&quot;) + coord_fixed() Ein so perfektes Ergebnis findet sich natürlich in Realität selten und sollte hier nur zur Veranschaulichung der verschiedenen Schritte einer Radiomics-Analyse dienen. 7.8 Das Ende. Vielen Dank! Wir hoffen, das Begleitbuch und unsere Webinare haben den einen oder anderen dazu bewogen sich intensiver mit den vielfältigen Möglichkeiten in R und RStudio zu beschäftigen. Natürlich könnte man noch viele anderen Themen bearbeiten und etwas realistischere und kompliziertere Radiomics- bzw. Machine Learning Anwendungen diskutieren. Wir haben aber versucht einigermaßen ein Gleichgewicht zwischen Praktikabilität und Praxisnähe zu finden und hoffen wir haben das einigermaßen erfolgreich gemacht. Über Kommentare, Rückfragen, Wünsche und Verbesserungsvorschläge freuen wir und sehr! Und sollte sich jemand in ähnlicher Weise engagieren wollen, vielleicht ja sogar mit einem ähnlichen Format zu Programmierung in Python, kann er sich gerne bei uns (Bettina Baeßler und Daniel Pinto dos Santos), der DRG oder der AGIT melden. In diesem Sinne beste Grüße und vielleicht bis bald! "]]
