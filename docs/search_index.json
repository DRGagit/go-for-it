[
["index.html", "DRG/AGIT - Go for IT Einleitung Termine der Live-Webinare 2019/2020 Zum Nachschauen", " DRG/AGIT - Go for IT Bettina Baeßler &amp; Daniel Pinto dos Santos 2020-03-03 Einleitung Big Data, Radiomics, Künstliche Intelligenz – diese IT-Themen sind in der radiologischen Community derzeit heiß diskutiert und stehen bei einer Vielzahl aktueller Fortbildungsveranstaltungen und Kongressen auf dem Programm. Zu Recht, denn sie gehen uns alle an – schließlich dürfte von ihnen abhängen, wie unsere Radiologie der Zukunft aussehen wird. Doch was steht eigentlich hinter solchen Begriffen wie „Big Data“ oder „Radiomics“? Wissen wir, wovon wir sprechen, wenn wir diese Begriffe benutzen, und sprechen wir eigentlich alle über dasselbe? Wie muss die Datenbasis von KI-Systemen aussehen, um aussagefähige Ergebnisse für den Patientennutzen zu bringen? Mit welchen Datensätzen und Algorithmen werden Radiologien künftig zu tun haben? Termine der Live-Webinare 2019/2020 25.03.2019 Theorie 01 - Einführungsveranstaltung 15.04.2019 R 02 - Einstieg in R: erste Schritte 17.06.2019 R 03 - Nächste Schritte in R: bunte Bilder und mehr 08.07.2019 Theorie 04 - Grundzüge in Statistik: sicher signifikant 02.09.2019 R 05 - Hands on – Deskriptive Statistik in R 30.09.2019 Theorie 06 - Wer Test sagt kann auch p sagen? Statistik überall 14.10.2019 R 07 - Theoretisch ja, praktisch auch! Tests in R 18.11.2019 R 08 - Diverse Tipps &amp; Tricks für R: Nützliche Pakete und Rmarkdown 09.12.2019 Theorie 09 - Genug gelernt, jetzt sind die Maschinen dran! Grundzüge Machine Learning 13.01.2020 Theorie 10 - Machine Learning: Test und Fehlermetriken 27.01.2020 R 11 - See one, do one, teach one! Machine Learning in R 10.02.2020 Theorie 12 - Wenn Bilder auch Daten sind: Einführung in Radiomics 16.03.2020 Theorie 13 - Weniger ist mehr: Feature Reduction in Radiomics 20.04.2020 R 14 - Bilddaten, Datenbilder: Radiomicsanalysen in R tba Theorie 15 - Die Oberfläche des tiefen Lernens: Grundzüge Deep Learning tba R 16 - A not so deep dive into deep learning: Keras in R Zum Nachschauen Alle Webinare sind auch als Aufzeichnung auf conrad (erfordert DRG-Login) zu finden. "],
["einstieg-in-r-erste-schritte.html", "1 Einstieg in R: erste Schritte 1.1 Lernziele 1.2 Installation von R und RStudio 1.3 Die RStudio Oberfläche 1.4 Arithmetische Operatoren 1.5 Logische Operatoren 1.6 Besondere Operatoren 1.7 Variablen 1.8 Funktionen 1.9 Vektoren 1.10 Dataframes 1.11 Daten einlesen 1.12 Ausblick 1.13 Schlussbemerkungen", " 1 Einstieg in R: erste Schritte Aller Anfang ist schwer, doch wohnt auch jedem Anfang ein Zauber inne (frei nach Hermann Hesse). Entsprechend sollte man sich nicht abschrecken lassen von der scheinbaren Komplexität und dem anfangs etwas unintuitiven Arbeiten. R erfordert sicher eine gewisse Umstellung, denn Analysen in anderen Statistik-Softwarepaketen “zusammenzuklicken” wirkt zunächst einfacher und komfortabler. Doch nicht nur für komplexere Auswertungen und Machine Learning, sondern auch für bessere Nachvollziehbarkeit und Reproduzierbarkeit eigener Auswertungen lohnt der Aufwand sich von graphischen Oberflächen zu lösen und ins kalte Wasser der “Programmierung” einzutauchen. Das Wort programmieren impliziert in gewisser Weise, dass eine Reihe von Befehlen aufgeschrieben wird, die dann vom Computer abgearbeitet werden. Genau so funktioniert R, indem bspw. der Befehl mean() die Anweisung repräsentiert, eine Reihe von Zahlen aufzusummieren und durch ihre Anzahl zu teilen. Dabei bietet R im Gegensatz zu anderen Programmiersprachen den Vorteil, dass die Befehle relativ gut für Menschen lesbar sind, und das eine Reihe von hilfreichen Paketen zur Verfügung stehen, die den Programmieraufwand für Anwendungen wie wir sie beispielsweise in der Medizin benötigen könnten relativ in Grenzen hält. Neben R existiert mit Python noch eine andere sehr weit verbreitete Programmiersprache, die sich in ähnlicher Weise nutzen lässt, im Funktionsumfang aber vermutlich sogar noch etwas mächtiger ist. Die Tatsache, dass wir hier R nutzen und präsentieren ist mehr dem Umstand geschuldet, dass wir R einfach selbst besser beherrschen. Wir befinden uns aber sicherlich nicht in schlechter Gesellschaft, denn auch große Unternehmen außerhalb der Medizin nutzen R für alle möglichen Datenanalysen. Companies that use R (Quelle DataFlair) 1.1 Lernziele Als Einstieg in R werden in diesem Kapitel die folgenden Themen besprochen: R und Rstudio installieren R als Taschenrechner benutzen Arithmetische, logische und besondere Operatoren in R verstehen Variablen erzeugen und mit ihnen arbeiten Funktionen in R benutzen und verstehen Datentabellen anlegen und Werte darin referenzieren Daten aus externen Dateien importieren sich einen Überblick über Daten verschaffen zusätzliche Programmbibliotheken laden gezielt nach Hilfe in Bezug auf Probleme in R suchen 1.2 Installation von R und RStudio Die Installation von R und RStudio sollte verhältnismäßig einfach und selbsterklärend sein. Wichtig zu bemerken ist, dass für diesen Kurs beide Programme benötigt werden. R ist die eigentliche Programmiersprache bzw. das Programm, dass unseren Programmcode letztlich ausführt, wohingegen RStudio eine Anwendung ist, die das Programmieren in R erheblich vereinfacht und viele praktische Funktionen anbietet. 1.2.1 Installation R Als erstes solle R installiert werden. Hierzu unter https://cran.r-project.org die passende Installationsdatei herunterladen und ausführen. R website (https://cran.r-project.org) 1.2.2 Installation RStudio Als nächstes kann dann RStudio installiert werden. Hierfür findet man die entsprechenden Installationsdateien unter https://rstudio.com/products/rstudio/. RStudio website (https://rstudio.com/products/rstudio/) Neben dem kostenlosesn RStudio Desktop bietet RStudio auch noch andere Produkte an für deren Nutzung eine Lizenzgebühr erhoben wird. Diese sind aber nur für professionelle Anwender von Interesse und für uns nicht nötig. 1.3 Die RStudio Oberfläche Nachdem alle nötigen Programme installiert sind, kann RStudio gestarten werden. Die Benutzeroberfläche wurde im Webinar (conrad) ausführlich erklärt. RStudio Oberfläche - über den roten Pfeil kann ein neues Projekt erstellt werden. Es ist von Vorteil sich innerhalb von RStudio sogenannte Projekte anzulegen. Diese stellen bspw. sicher, dass Dateiausgaben eines R-Skriptes im richtigen Ordner landen, bzw. Dateien, die eingelesen werden sollen - wenn sie sich im Projektordner befinden - leichter referenziert werden können. Projekte können über den Button links oben in RStudio erstellt werden, alternativ via File -&gt; New Project.... Üblicherweise sollte beim Start bereits ein leeres Skript im Editorabschnitt der Benutzeroberfläche zu sehen sein. Falls nicht sollte über File -&gt; New File -&gt; R Script ein leeres Skript erzeugt werden. 1.4 Arithmetische Operatoren R kann im einfachsten Falle als überdimensionierter Taschenrechner benutzt werden. Hierzu entweder im Editorbereich den entsprechenden Code eingeben und mittels cmd+enter bzw. strg+enter ausführen oder direkt im Konsolenbereich eingeben und mit enter ausführen. # Addition 3+5 ## [1] 8 # Subtraktion 7-2 ## [1] 5 # Multiplikation 3*4 ## [1] 12 # Division 16/2 ## [1] 8 # Potenzen 2**3 ## [1] 8 Achtung: In R gilt der Punkt . als Dezimaltrenner, entsprechend werden Berechnungen in denen ein Komma , als Dezimaltrenner steht mit einem Fehler quittiert. 1.2 * 3.4 # funktioniert ## [1] 4.08 # 1,2 * 3,4 würde nicht funktionieren und einen Fehler ausgeben!! 1.5 Logische Operatoren Logische Operatoren führen bspw. Vergleiche durch, als Ergebnis wird ein sog. Boolscher Wert zurückgegeben, d.h. entweder wahr TRUE oder falsch FALSE. Man kann es sich wie eine Frage vorstellen, die entsprechend mit ja oder nein beantwortet wird. # größer als 2 &gt; 1 # also etwa die Frage: &quot;Ist 2 größer als 1? - Antwort: &quot;Ja!&quot; ## [1] TRUE # kleiner als 2 &lt; 2 ## [1] FALSE # kleiner gleich (entsprechend &gt;= für größer gleich) 2 &lt;= 2 ## [1] TRUE # gleich 5 == 7 ## [1] FALSE # ungleich 5 != 7 ## [1] TRUE 1.6 Besondere Operatoren Es existieren eine Reihe von besonderen Operatoren, die in gewisser Weise einfache Funktionen sind und bestimmte Effekte erzeugen. Der einfachste besondere Operator ist der Doppelpunkt :. Dieser erzeugt eine Zahlenreihe, beginnend bei der linken Zahl und dann in Schritten von 1 zur rechten Zahl gehend. # Reihenoperator 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 # würde die rechte Zahl mit dem nächsten Schritt überschritten, # wird der letzte Schritt bspw. kleiner als die rechte Zahl sein 1.6:10.1 ## [1] 1.6 2.6 3.6 4.6 5.6 6.6 7.6 8.6 9.6 1.7 Variablen Wirklich spannend wird Programmierung aber erst dann, wenn Werte nicht immer wieder ausgeschrieben werden müssen, sondern vor allem, wenn statt einzelner Werte Variablen erzeugt werden, mit denen dann wiederum weitergearbeitet werden kann, unabhängig von deren genauem Wert. So kann eine Abfolge von Befehlen immer und immer wieder genutzt werden, und erzeugt in Abhängigkeit der Variablenwerte entsprechend unterschiedliche Ergebnisse. In R werden Variablen mithilfe des Zuweisungsoperators &lt;- erzeugt bzw. der Wert einer Variablen zugewiesen. Der Name der Variablen kann dabei frei gewählt werden, muss aber mit einem Buchstaben beginnen. # Zuweisungsoperator &#39;&lt;-&#39; weist rechten Wert der links genannten Variablen zu a &lt;- 9.385 wolke &lt;- 5.772 # entsprechend kann dann mit diesen Variablen gerechnet werden a * wolke ## [1] 54.17022 # Ergebnisse können ebenfalls Variablen zugewiesen werden ergebnis &lt;- a * wolke # gibt man lediglich den Variablennamen an und führt diese Codezeile aus, # erhält man den Wert der Variablen zurück ergebnis ## [1] 54.17022 1.8 Funktionen Funktionen wiederum sind kurze Befehle, die im Hintergrund eine Reihe von Operationen durchführen. Eine einfache Funktion wäre bspw. der oben erwähnte Befehl mean(). Funktionen haben für gewöhnlich eine Reihe von Parametern, die innerhalb der Klammern () übergeben werden. Diese können bspw. Werte sein, die von der Funktion verarbeitet werden, oder auch Werte, die die Funktionsweise eines Befehls modifizieren. # Werte können als Parameter an Funktionen übergeben werden # Mittelwert mean(1:10) ## [1] 5.5 # Runden, erster Parameter wird auf die im zweiten Parameter angegebene Stellen gerundet round(2.1271, 2) ## [1] 2.13 # Parameter können auch Variablen sein round(ergebnis, 1) ## [1] 54.2 1.9 Vektoren In R werden mit Vektoren Aneinanderreihungen von Werten bezeichnet. Diese können entweder Zahlen enthalten oder bspw. auch Zeichenketten aus Buchstaben. Wichtig ist bei Vektoren jedoch, dass alle Werte vom gleichen Typ sein müssen. D.h. entweder sind alle Elemente des Vektors Zahlen oder bspw. Zeichenketten (die wiederum sowohl Buchstaben als auch Zahlen enthalten können, allerdings kann mit den Zahlen dann nicht gerechnet werden, weil R sie wie Text behandelt). Vektoren können mit der Funktion c() erschaffen werden. Elemente innerhalb eines Vektors können mit ihrem Index in eckigen Klammern [] direkt angesprochen werden. vektor &lt;- c(2,3,5,7,9,10) # ein Vektor aus Zahlen # Ausgabe des gesamten Vektors vektor ## [1] 2 3 5 7 9 10 # Ausgabe eines Elements innerhalb eines Vektors, der Index # in den eckigen Klammern entspricht der Position im Vektor vektor[3] ## [1] 5 Mit Vektoren kann ebenso gerechnet werden, wie mit normalen Zahlen. R wendet dabei die Rechenoperation auf jedes Element des Vektors an. Ebenso können mithilfe des Zuweisungsoperators &lt;- einzelne Positionen im Vektor überschrieben werden. Und natürlich können auch Vektoren an Funktionen übergeben werden. vektor[5] &lt;- 14 # überschreibt den Wert 9, der bisher an Position 5 stand mit dem Wert 14 vektor ## [1] 2 3 5 7 14 10 vektor * 2 # jedes Element des Vektors wird mit 2 multiplizert ## [1] 4 6 10 14 28 20 # es können auch Vektoren miteinander multipliziert werden, dabei # werden beide Vektoren durchgegangen, d.h. 1. Element mit 1. Element, # 2. Element mit 2. Element, usw. - haben die Vektoren unterschiedliche # Längen wird beim kürzeren Vektor wieder von vorne begonnen vektor * c(1,2,3) ## [1] 2 6 15 7 28 30 # der Vektor kann an Funktionen übergeben werden mean(vektor) ## [1] 6.833333 1.10 Dataframes Aber so richtig Spaß macht R erst, wenn man größere Datenmengen manipulieren kann. Außerdem braucht man ein Konstrukt indem Bezüge von einzelnen Daten klar sind. Beispielsweise macht es keinen Sinn einen Vektor mit DLP-Werten von CT-Untersuchungen zu haben und einen mit CTDI-Werten und einen anderen mit Untersuchungsbeschreibungen, wenn nicht klar ist, welches CTDI zu welchem DLP gehört. Hierzu bietet R ein Objekt an, das im Weiteren für die allermeisten Anwendungsfälle der Dreh- und Angelpunkt sein wird: das Dataframe. Dataframes kann man sich im Grunde wie Excel-Tabellen vorstellen. Es gibt Zeilen und Spalten, wobei dem Konzept der “tidy data” folgend, jede Zeile eine “Beobachtung” sein sollte und jede Spalte die verschiedenen Variablen für die Beobachtungen enthält. Für ein einfaches Beispiel erstellen wir ein Dataframe mit sechs CT-Untersuchungen (= sechs Beobachtungen) und drei Variablen (= Spalten), nämlich Untersuchungs-ID, Untersuchungsbezeichnung und DLP. Hierfür nutzen wir die Funktion data.frame() und übergeben als Parameter die jeweiligen Spalten, wobei die Werte jeder Spalte als Vektor übergeben werden. # da R Zeilenumbrüche nach Klammern und Kommas ignoriert, können wir etwas # besser lesbaren Code erzeugen, indem wir Funktion und Spalten jeweils # in eigene Zeilen schreiben tabelle &lt;- data.frame( id = c(1, 2, 3, 4, 5), unt = c(&quot;thx&quot;, &quot;abd&quot;, &quot;thx&quot;, &quot;thx&quot;, &quot;abd&quot;), dlp = c(200, 350, 210, 220, 340) ) tabelle ## id unt dlp ## 1 1 thx 200 ## 2 2 abd 350 ## 3 3 thx 210 ## 4 4 thx 220 ## 5 5 abd 340 Ähnlich wie bei Vektoren können wir auch einzelne Werte mithilfe von eckigen Klammern [] referenzieren. Dabei müssen aber für Einzelwerte sowohl Zeile als auch Spalte angegeben werden. Fehlt die Angabe für Zeile oder Spalte wird jeweils die gesamte Zeile bzw. Spalte referenziert. Dabei steht innerhalb der eckigen Klammer [] zuerst die Zeile, dann gefolgt von einem Komma , die Spalte. # holt Wert aus zweiter Zeile, dritter Spalte tabelle[2,3] ## [1] 350 # holt gesamte erste Zeile tabelle[1,] ## id unt dlp ## 1 1 thx 200 # gibt gesamte zweite Spalte aus tabelle[,2] ## [1] thx abd thx thx abd ## Levels: abd thx Da Spalten üblicherweise mit Spaltennamen versehen sind, bietet R auch die etwas einfachere Möglichkeit Spalten mit ihrem Namen anzusprechen, indem man das Dollarzeichens $ nutzt. Der Rückgabewert ist hierbei einfach ein Vektor mit allen Werten der entsprechenden Spalte. tabelle$dlp ## [1] 200 350 210 220 340 Natürlich lassen sich entsprechend auch Werte an entsprechenden Positionen mithilfe des Zuweisungsoperators &lt;- überschreiben, und logischerweise kann man auch mit den Werten aus Dataframes rechnen. # überschreibt den Werte der dritten Spalte in der fünften Zeile tabelle[5,3] &lt;- 500 # berechnet Mittelwert der DLPs mean(tabelle$dlp) ## [1] 296 Eine sehr nützliche Funktion um einen ersten Überblick über ein Dataframe zu bekommen ist die Funktion str(), die die Struktur eines Dataframes ausgibt. str(tabelle) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ id : num 1 2 3 4 5 ## $ unt: Factor w/ 2 levels &quot;abd&quot;,&quot;thx&quot;: 2 1 2 2 1 ## $ dlp: num 200 350 210 220 500 1.11 Daten einlesen Noch viel interessanter werden die Möglichkeiten in R, wenn man Daten aus üblichen Formaten einlesen kann und dann entsprechend damit in R weiterarbeiten kann. Es existieren natürlich auch Möglichkeiten Excel-Dateien direkt einzulesen, da das aber manchmal zu unerwarteten Problemen führen kann, lohnt es sich zumeist die Daten in Excel als CSV-Datei zu speichern und diese dann in R einzulesen. Normalerweise werden in CSV-Dateien, wie der Name schon sagt, Werte mit Komma , getrennt. Da wir aber üblicherweise das Komma als Dezimaltrenner nutzen, werden in den Dateien die Werte anders als gewöhnlich mit Semikolons ; getrennt und das Komma als Dezimaltrenner genutzt. Damit R nicht durcheinandergerät, muss diese Information der einlesenden Funktion read.csv() als Parameter mitgeteilt werden. Alternativ steht eine zweite Funktion bereit read.csv2(), die bereits von der Situation Semikolon-getrennter Werte ausgeht und daher diese Parameter nicht mehr benötigt. Für das folgende und einige weitere Beispiele im Verlauf wird die Datei ct_data.csv benötigt (~Download~). Am einfachsten ist es diese Datei herunterzuladen und anschließend im lokalen Projektordner zu speichern. # alternativ ginge auch einfach read.csv2(&quot;ct_data.csv&quot;) daten &lt;- read.csv(&quot;ct_data.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) str(daten) ## &#39;data.frame&#39;: 2441 obs. of 6 variables: ## $ tag : Factor w/ 207 levels &quot;2018-01-01&quot;,&quot;2018-01-02&quot;,..: 1 1 1 1 1 1 1 1 1 2 ... ## $ untersuchung: Factor w/ 6 levels &quot;Abdomen&quot;,&quot;Becken&quot;,..: 6 1 1 5 5 1 4 4 6 4 ... ## $ kv : int 120 120 120 120 120 120 100 100 120 100 ... ## $ mas : num 250 142 105 82 179 50 189 189 250 189 ... ## $ ctdi.vol : num 16.9 9.7 7.1 5.6 8 3.4 7.7 7.7 16.9 7.7 ... ## $ dlp : num 3270 1255 454 1221 353 ... 1.12 Ausblick Sind Daten erstmal eingelesen, können mit wenigen Befehlen komplexe Analysen gemacht werden und Daten ansprechend visualisiert werden. Vieles davon wird in den folgenden Kapiteln erklärt. Der folgende Code soll daher nur einen kleinen Ausblick liefern und Appetit machen. library(ggplot2) library(ggrepel) ggplot(data = daten, aes(x = ctdi.vol, y = dlp, label = as.character(tag))) + geom_point(aes(color=untersuchung)) + geom_smooth(method=lm, se=FALSE, color = &quot;#CCCCCC&quot;, linetype = &quot;dashed&quot;) + geom_text_repel(data = subset(daten, ctdi.vol &gt; 35), nudge_x = 15, direction = &quot;y&quot;) + geom_text_repel(data = subset(daten, dlp &gt; 5370), nudge_y = 1000, direction = &quot;x&quot;) + scale_x_continuous(limits = c(0, 45)) + scale_y_continuous(limits = c(0, 6300)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + labs(title = &quot;Punktwolke für DLP in Abhängigkeit von CTDI&quot;, x = &quot;DLP&quot;, y = &quot;CTDI&quot;, color = &quot;Untersuchungsbezeichnung&quot;) 1.13 Schlussbemerkungen Sicherlich ist man im ersten Moment erschrocken von der Fülle der Möglichkeiten und der gleichzeitigen Abwesenheit eines Leitfadens. Keine Menüs in denen man einfach mal nach der passenden Funktion suchen kann. Aber glücklicherweise ist die R Community sehr aktiv, und man wird selbst für die banalsten Probleme mit einer kurzen Google-Suche fündig. Dabei sollte man allerdings besser direkt auf Englisch suchen, weil dies zweifelsohne die Chance erhöht eine Antwort zu finden. Zum Beispiel: \"Wie erstelle ich ein dataframe?\" bzw. \"How to build a dataframe?\". Sicherlich auch empfehlenswert, um das in diesem Kapitel erlernte weiter zu vertiefen, sei an dieser Stelle das Paket Swirl genannt, dass auf spielerische Art und Weise eine Einführung in R vermittelt. "],
["nächste-schritte-in-r-bunte-bilder-und-mehr.html", "2 Nächste Schritte in R: bunte Bilder und mehr 2.1 Lernziele 2.2 Grafiken in Base R 2.3 Pakete installieren und laden 2.4 Grafiken mit ggplot2", " 2 Nächste Schritte in R: bunte Bilder und mehr Das Sprichwort “Ein Bild sagt mehr als tausend Worte” stimmt auch und insbesondere, wenn man sich mit Daten beschäftigt. Häufig ist es wesentlich einfacher, sich mithilfe einer guten Visualisierung einen ersten Überblick über Daten zu verschaffen, als anhand relativ abstrakter Maßzahlen, wie beispielsweise Mittelwert, Median, Quartilen, etc. Noch deutlicher wird es, wenn man beispielsweise grobe Zusammenhänge in Daten finden will. Da wir sehr gewohnt sind, mit unseren Augen Muster zu erkennen, können wir in einer Grafik beispielsweise extrem schnell Gruppen und Ausreißer identifizieren, was bei bloßer Betrachung einer Tabelle schier unmöglich wäre. Beipsiel einer einfachen Punktwolke: Leicht erkennt man eine Gruppe von Untersuchungen mit identischem CTDI und dass 80kV-Untersuchungen zu den DLP-Ausreißern gehören. In vielen Anwendungsfällen stellt daher die Datenvisualisierung einen ersten wichtigen Schritt dar. 2.1 Lernziele In diesem Kapitel werden folgende Themen besprochen: Grafiken mithilfe von R-Basisfunktionen zu erstellen zusätzliche Pakete installieren und laden die Syntax von ggplot2 verstehen und Grafiken damit erstellen Punktwolke Balkendiagramm Histogramm / Verteilungsdichte Boxplot 2.2 Grafiken in Base R Die Grafikfunktionen in Basis-R sind verhältnismäßig schnell erlernbar und einfach, allerdings bieten sie nur einen recht beschränkten Funktionsumfang. Trotzdem lohnt es sich, sich auch mit diesen zu beschäftigen, denn für einfache schnelle Auswertungen eignen sie sich ganz gut. Nachdem wir im vorigen Kapitel gesehen haben, wie man einzelne Spalten eines Dataframes gezielt anspricht, können wir aus unserer Datei ct_data.csv (~Download~) beispielsweise eine einfache Grafik erstellen, die DLP gegen CTDI aufträgt. plot(daten$ctdi.vol, daten$dlp) Die Funktion plot() erwartet als Parameter zwei Vektoren gleicher Länge, die die x- und y-Werte repräsentieren. Es ist also nicht zwingend nötig, die Grafik aus einem Dataframe zu erzeugen. Ähnlich einfach können Boxplots und Balkendiagramme erstellt werden. hist(daten$dlp) boxplot(daten$dlp ~ daten$untersuchung) Zwar können auch mit Basis-R erstellte Grafiken optisch ansprechender gestaltet werden, doch ist die Syntax der Befehle an vielen Stellen etwas unintuitiv, weshalb an dieser Stelle nicht weiter darauf eingegangen werden soll. 2.3 Pakete installieren und laden Glücklicherweise hat man in R die Möglichkeit zusätzliche Pakete zu installieren, die den Funktionsumfang erheblich erweitern. Ein solches Paket (oder besser gesagt gleich eine ganze Sammlung solcher Pakete) ist das Tidyverse. Über die Vor- und Nachteile der Benutzung von Funktionen aus dem Tidyverse wird im Internet in regelmäßigen Abständen hitzig debatiert (z.B. contra und pro). In meiner persönlichen Arbeitsweise ist das Tidyverse eine unverzichtbare Säule und fast immer das Laden desselben die erste Zeile Code. Neben Funktionen zur Datenaufbereitung und -bearbeitung enthält das Tidyverse das äußerst umfangreiche ggplot2-Paket, das bei der Erstellung von Grafiken hilft. Doch von vorne. Zur Installation des tidyverse kann man entweder die graphische Oberfläche von RStudio nutzen und dieses über Tools -&gt; Install Packages... installieren oder den entsprechenden Button im Packages-Pane nutzen. Alternativ kann man auch über die Konsole das Paket installieren. install.packages(&quot;tidyverse&quot;) 2.3.1 Ein kurzer Ausflug ins Tidyverse Bevor wir zu den “bunten Bildern” kommen, vielleicht noch ein kurzer Ausflug ins Tidyverse, um einige Feinheiten dieses Paketes zu erläutern. Alle dazugehörigen Pakete folgen der Philosphie der “tidy data”. Idealerweise sollte man sich diese Philosophie im Umgang mit Daten angewöhnen, da es erheblich zur Klarheit der Daten beiträgt und natürlich auch entsprechend die weiteren Auswertungen in R erleichtert. Um alle Pakete des Tidyverse zu laden, reicht ein einfacher Befehl. # die Meldungen, die hierbei auf der Konsole ausgegeben werden, # können erstmal ignoriert werden library(tidyverse) Darüberhinaus enthält das Tidyverse verschiedene Funktionen, die Funktionen aus Basis-R sehr ähnlich sind, aber einige smarte Vorteile bergen. So existiert bspw. zum Einlesen von Daten die Funktion read_csv2(), die nicht nur im Namen nahezu identisch ist mit der bereits bekannten Funktion read.csv2(). Allerdings hat die tidyverse-Funktion read_csv2() den angenehmen Vorteil, dass sie u.a. versucht zu erkennen, ob in einer Spalte nur Daten eines Typus sind (z.B.: Text, Zahlen oder Datumsangaben) und dann direkt ein Dataframe mit korrekter Formatierung anlegt. # die read_csv2() Funktion gibt Meldungen aus, die erinnern sollen mit welchen # Einstellungen die Daten eingelesen wurden und welche Datentypen erkannt wurden. daten &lt;- read_csv2(&quot;ct_data.csv&quot;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## tag = col_date(format = &quot;&quot;), ## untersuchung = col_character(), ## kv = col_double(), ## mas = col_double(), ## ctdi.vol = col_double(), ## dlp = col_double() ## ) (Es mag an dieser Stelle viellicht noch nicht ganz klar sein, was damit gemeint ist. Die Vorteile werden aber im Verlauf vielleicht offensichtlich. Ich würde daher empfehlen, immer die Tidyverse-Entsprechungen von Base-R Funktionen zu nutzen). Eine weitere essentielle Funktion Erweiterung, die das Tidyverse mit sich bringt, ist der Übergabeoperator %&gt;%. Dieser erlaubt es, mehrere Befehle in für Menschen leichter lesbarer Weise miteinander zu verketten, anstatt dass kompliziert verschachtelte Funktionen benutzt werden müssen. Vereinfacht kann gesagt werden, dass jeweils das Ergebnis einer Funktion als erster Parameter an die folgende Funktion weitergereicht wird. Am einfachsten lässe es sich an einem banalen Beispiel verdeutlichen: aus einer Reihe Zahlen soll zunächst der Mittelwert gebildet werden, dieser soll anschließend auf drei Stellen nach dem Komma gerundet werden. # in Basis-R ergäbe sich dieser verschachtelte Befehl # es ist nicht leicht zu erkennen, dass die Funktion mean() hier # der erste Parameter für die Funktion round() ist, die 3 am Ende # der zweite round(mean(c(12.3432345, 5.834242453, 9.73389543)), 3) ## [1] 9.304 # gleicher Befehl in Tidyverse-Syntax # es kann einfach von links nach rechts eine Kette von Befehlen # gelesen werden c(12.3432345, 5.834242453, 9.73389543) %&gt;% mean() %&gt;% round(3) ## [1] 9.304 2.4 Grafiken mit ggplot2 Um Grafiken mit dem Paket ggplot2, das auch Teil des Tidyverse ist, zu erstellen muss man zunächst grob die zugrundeliegende “Grammatik der Grafiken” verstehen. Die Idee ist, dass ähnlich wie in einem Satz bspw. Subjekt, Verb und Objekt stehen, Grafiken in der Form Daten, Anordnung der Daten und Aussehen der Grafik beschreiben lassen. Grammatik der Grafiken. Dabei ist zu beachten, dass die Funktion ggplot() das Erstellen von Grafiken startet und als ersten Parameter ein Dataframe erwartet. Anschließend muss innerhalb von ggplot() die ästhetische Eigenschaft definiert werden, also bspw. welche Werte auf die x-Achse und welche auf die y-Achse sollen. Dann können mit + jeweils Objekte und Elemente der Grafik hinzugefügt werden. Zwar sind dann im Einzelfall gelegentlich mehr Befehle nötig als in Basis-R, dafür stehen zahlreiche Möglichkeiten zur Verfügung nahezu jedes Element der Grafik bis ins Detail an die eigenen Wünsche anzupassen. Wir nehemen als erstes Beispiel die Punktwolke von oben. ggplot(daten, aes(x = ctdi.vol, y = dlp)) + geom_point() In ähnlicher Weise können auch das Histogramm und der Boxplot reproduziert werden. Außerdem kann natürlich auch das Dataframe als erster Parameter mithilfe des Verkettungsoperators %&gt;% weitergegeben werden. # für das Histogramm wird keine Angabe zur y-Achse benötigt, # dies wird aus der Häufigkeit der Werte berechnet daten %&gt;% ggplot(aes(x = dlp)) + geom_histogram() daten %&gt;% ggplot(aes(x = untersuchung, y = dlp)) + geom_boxplot() Es können nicht nur ästhetische Eigenschaften im Sinne von x- und y-Zuordnung definiert werden, sondern beispielsweise auch die Farbe eines Elements. Im folgenden Beispiel wollen wir die Punkte in der Punktwolke einmal nach Untersuchungsart einfärben. daten %&gt;% ggplot(aes(x = ctdi.vol, y = dlp, color = untersuchung)) + geom_point() Grafiken können ebenfalls in Variablen gespeichert werden und dann weiter mit + um zusätzliche Elemente ergänzt werden. Im folgenden Beispiel wird die Punktwolke mit den farbigen Punkten in einer Variablen gespeichert. Anschließend wird sie mit besseren Beschriftungen versehen und wiederum gespeichert, zuletzt wird ein anderes Aussehen mithilfe der theme_bw() Funktion auf die Grafik angewendet (neben theme_bw() existieren noch weitere wie theme_minimal(), theme_dark(), usw.). buntes.bild &lt;- daten %&gt;% ggplot(aes(x = ctdi.vol, y = dlp, color = untersuchung)) + geom_point() buntes.bild.beschriftet &lt;- buntes.bild + labs(title = &quot;DLP vs. CTDI&quot;, y = &quot;DLP [mGy*cm]&quot;, x = &quot;CTDI [mGy]&quot;, color = &quot;Untersuchungsbezeichnung&quot;) buntes.bild.beschriftet + theme_bw() "],
["deskriptive-statistik-in-r.html", "3 Deskriptive Statistik in R 3.1 Lernziele 3.2 Deskriptive Statistiken mit Basis R 3.3 Deskriptive Statistiken im Tidyverse 3.4 Deskriptive Statistiken mit anderen Paketen 3.5 Mehr bunte Bilder", " 3 Deskriptive Statistik in R Visualisierung ist natürlich schon gut um einen ersten schnellen Überblick über Daten zu bekommen, aber früher oder später geht es ans Eingemachte und es müssen harte und genaue Zahlen her. Einige der wichtigen Zahlen, die unsere Daten beschreiben sind sogenannte deskriptive Statistiken. Darunter fallen so einfache Dinge wie der Mittelwert, aber auch andere Werte wie Standardabweichung, Median, Minimum, Maximum und Quartile oder auch seltener gebrauchte Werte wie der Modus, Kurtosis und Varianz. 3.1 Lernziele In diesem Kapitel werden folgende Themen besprochen: deskriptive Statistiken in R erstellen nach Gruppen aufgeteilte deskriptive Statistiken erstellen „tidyverse“-Funktionen nutzen, um tiefere Einblicke in die Daten zu erhalten Boxplots erstellen und verstehen nach Gruppen getrennte Grafiken erstellen 3.2 Deskriptive Statistiken mit Basis R Erfreulicherweise lassen sich die meisten der Werte für einfache deskriptive Statistiken mit Basis R Funktionen berechnen. Die entsprechenden Funktionen sind nach dem zu berechnenden Wert benannt und im Grunde selbsterklärend. min(daten$dlp) ## [1] 1.2 mean(daten$dlp) ## [1] 1126.547 median(daten$dlp) ## [1] 576 max(daten$dlp) ## [1] 5979.3 Es wäre natürlich insbesondere in großen Tabellen müßig, jede Spalte einzeln aufzurufen. Die Funktion summary() hilft hier und berechnet diese Werte und einige mehr gleich für alle Spalten eines Dataframes. Besonders praktisch hieran: für Spalten, die keine numerischen Werte enthalten, erhält man trotzdem einige praktische quantitative Werte. summary(daten) ## tag untersuchung kv mas ## Min. :2018-01-01 Length:2441 Min. : 12.0 Min. : 2.0 ## 1st Qu.:2018-02-24 Class :character 1st Qu.:120.0 1st Qu.: 92.0 ## Median :2018-04-13 Mode :character Median :120.0 Median : 150.0 ## Mean :2018-04-14 Mean :114.5 Mean : 156.9 ## 3rd Qu.:2018-06-04 3rd Qu.:120.0 3rd Qu.: 206.0 ## Max. :2018-07-31 Max. :801.0 Max. :1231.0 ## ctdi.vol dlp ## Min. : 0.05 Min. : 1.2 ## 1st Qu.: 5.50 1st Qu.: 244.6 ## Median : 8.00 Median : 576.0 ## Mean :10.30 Mean :1126.5 ## 3rd Qu.:14.30 3rd Qu.:1795.3 ## Max. :39.60 Max. :5979.3 Mit den zwei Funktionen str(), die uns einen Überblick über die Struktur eines Dataframes gibt, und der summary() Funktion ergibt sich in den meisten Fällen ein recht guter Eindruck der vorliegenden Daten. Aber oftmals ist nicht so sehr der Überblick über die gesamten Daten gefragt, vielmehr wären nach Gruppen getrennte deskriptive Statistiken von Interesse. Hierzu bietet Basis R die Funktion by, die in der Benutzung zwar etwas unintuitiv ist, aber genau diese Funktionsweise abbildet. Als ersten Parameter erwartet by() einen Vektor (meist also eine Spalte eines Dataframes), auf den nach dem als zweiten Parameter übergebenen Gruppenvektor die als dritter Parameter übergebene Funktion angewandt wird. # erster Parameter: die zu untersuchenden Werte # zweiter Parameter: die Gruppenvariable # dritter Parameter: die anzuwendende Funktion (ohne runde Klammern!) by(daten$dlp, daten$untersuchung, median) ## daten$untersuchung: Abdomen ## [1] 714.85 ## ------------------------------------------------------------ ## daten$untersuchung: Becken ## [1] 341.8 ## ------------------------------------------------------------ ## daten$untersuchung: Gesicht ## [1] 379.4 ## ------------------------------------------------------------ ## daten$untersuchung: Kopf ## [1] 1433 ## ------------------------------------------------------------ ## daten$untersuchung: Thorax ## [1] 219 ## ------------------------------------------------------------ ## daten$untersuchung: Traumaspirale ## [1] 2674.9 # die Ausgabe kann bei vielen Gruppen und Funktionen, die lange # Ausgaben produzieren mitunter recht unübersichtlich werden. # Die Ausgabe dieses Befehls wird deshalb hier nicht gezeigt. by(daten, daten$untersuchung, summary) 3.3 Deskriptive Statistiken im Tidyverse Das Berechnen deskriptiver Statistiken ist keine Aufgabe, für die im Tidyverse per se spezielle Funktionen bereitstehen. Trotzdem bieten einige Funktionen umfangreiche Möglichkeiten fast spielerisch mit den Daten zu interagieren und einfach und schnell Daten zu selektieren, zu gruppieren und auszuwerten. library(tidyverse) Die wichtigsten Funktionen sind hierfür die Funktionen select() (wählt Spalten aus), filter() (wählt Zeilen aus), group_by() (bildet Gruppen) und summarise() (führt Berechnungen für Gruppen aus). Mithilfe von arrange() können wir die entstehende Ausgabe auch noch nach einer Spalte sortieren. Diese Funktionen lassen sich in nahezu beliebiger Weise mit %&gt;% verketten und sind trotzdem relativ einfach zu lesen. Für ein erstes Beispiel wollen wir zum Beispiel unsere Daten nach Untersuchungsbeschreibung gruppieren, dann den Median von CTDI (in mGy wie in der ursprünglichen Tabelle und direkt auch durch 10 geteilt in cGy) und DLP berechnen, sowie die Anzahl der jeweiligen Untersuchungen. Und um die Daten noch leichter fassbar zu machen, soll die Ausgabe nach dem Median des CTDI sortiert werden. Der Übersichtlichkeit halber lohnt es sich statt lange Zeilen zu schreiben, die einzelnen Teile der Befehle auf mehrere Zeilen aufzuteilen. daten %&gt;% group_by(untersuchung) %&gt;% summarise( median_ctdi_mGy = median(ctdi.vol), median_ctdi_cGy = median(ctdi.vol) / 10, median_dlp = median(dlp), anzahl = n() ) %&gt;% arrange(-median_ctdi_mGy) ## # A tibble: 6 x 5 ## untersuchung median_ctdi_mGy median_ctdi_cGy median_dlp anzahl ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Traumaspirale 16.9 1.69 2675. 273 ## 2 Gesicht 12.2 1.22 379. 97 ## 3 Abdomen 10.0 1.01 715. 682 ## 4 Becken 9.3 0.93 342. 49 ## 5 Kopf 7.7 0.77 1433 523 ## 6 Thorax 4.8 0.48 219 817 In ähnlicher Weise könnten wir auch bspw. zunächst die Daten nach Untersuchungen filtern, bei denen genau 100kV am Gerät eingestellt waren, dann einige der Werte aus dem letzten Beispiel berechnen und zuletzt in der Ausgabe nur diejenigen Untersuchungensbezeichnungen auflisten, die weniger als 100 mal mit 100kV durchgeführt wurden. Zu guter letzt könnte man auch noch die Spalte Anzahl aus der Ausgabe entfernen, sodass nur die berechneten Werte bleiben. daten %&gt;% filter(kv == 100) %&gt;% group_by(untersuchung) %&gt;% summarise( median_ctdi_mGy = median(ctdi.vol), median_dlp = median(dlp), anzahl = n() ) %&gt;% filter(anzahl &lt; 100) %&gt;% select(-anzahl) ## # A tibble: 4 x 3 ## untersuchung median_ctdi_mGy median_dlp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abdomen 5.95 796. ## 2 Becken 11.2 474. ## 3 Gesicht 12.2 1194. ## 4 Traumaspirale 10.7 2448. Die Stärke des Tidyverse liegt hier sicher in der Möglichkeit, interaktiv Befehle zu verketten. Man schreibt einige Befehle auf, von denen man glaubt, dass sie das gewünschte Ergebnis bringen könnten oder den Weg dorthin darstellen, führt diese aus und betrachtet die Ausgabe. Dann passt man die Befehle an oder erweitert die Kette um weitere bis man schließlich das gewünschte Ergebnis erhält. 3.4 Deskriptive Statistiken mit anderen Paketen Wie bereits erwähnt, ist eine der herausragenden Eigenschaften von R, dass unzählige Pakte existieren, die verschiedene Funktionen bereitstellen und so das Arbeiten vereinfachen. Zwei sehr gute Pakete für deskriptive Statisiken sind psych und summarytools. Diese können, wie Tidyverse im vorherigen Kapitel, über die entsprechenden Funktionen von RStudio oder über die Konsole installiert werden. 3.4.1 Psych Aus dem Paket psych können wir die Funktion describe() benutzen, um eine Vielzahl von Maßzahlen zu berechnen. library(psych) describe(daten) ## vars n mean sd median trimmed mad min max ## tag 1 2441 NaN NA NA NaN NA Inf -Inf ## untersuchung* 2 2441 NaN NA NA NaN NA Inf -Inf ## kv 3 2441 114.47 18.77 120 117.36 0.00 12.00 801.0 ## mas 4 2441 156.93 92.65 150 150.06 85.99 2.00 1231.0 ## ctdi.vol 5 2441 10.30 6.98 8 9.44 5.93 0.05 39.6 ## dlp 6 2441 1126.55 1142.93 576 960.73 622.10 1.20 5979.3 ## range skew kurtosis se ## tag -Inf NA NA NA ## untersuchung* -Inf NA NA NA ## kv 789.00 19.45 731.66 0.38 ## mas 1229.00 1.79 13.34 1.88 ## ctdi.vol 39.55 1.21 1.40 0.14 ## dlp 5978.10 1.18 0.51 23.13 Die üblichen Quartile erhält man, wenn man der describe()-Funktion einen entsprechenden zusätzlichen Parameter übergibt. Ebenfalls kann man describe() mitteilen, dass nicht-numerische Werte übersprungen werden sollen. describe(daten, quant=c(.25,.75), omit = TRUE) ## vars n mean sd median trimmed mad min max range ## kv 3 2441 114.47 18.77 120 117.36 0.00 12.00 801.0 789.00 ## mas 4 2441 156.93 92.65 150 150.06 85.99 2.00 1231.0 1229.00 ## ctdi.vol 5 2441 10.30 6.98 8 9.44 5.93 0.05 39.6 39.55 ## dlp 6 2441 1126.55 1142.93 576 960.73 622.10 1.20 5979.3 5978.10 ## skew kurtosis se Q0.25 Q0.75 ## kv 19.45 731.66 0.38 120.0 120.0 ## mas 1.79 13.34 1.88 92.0 206.0 ## ctdi.vol 1.21 1.40 0.14 5.5 14.3 ## dlp 1.18 0.51 23.13 244.6 1795.3 Eine ebenfalls sehr praktische Funktion ist die der Base-R Funktion by() (s.o.) nachempfundenen describe.by() Funktion. Dieser können die selben Parameter mitgegeben werden, wie der einfachen describe() Funktion. # Da auch hier die Ausgabe sehr lang würde, wird sie hier nicht abgebildet. describe.by(daten, daten$untersuchung, quant=c(.25,.75), omit = TRUE) 3.4.2 Summarytools Auch im Paket summarytools existieren Funktionen, die ähnliche Ausgaben erzeugen wie die Funktion describe() aus dem psych Paket. Welche Pakete man letztenendes benutzen will, ist Frage des persönlichen Geschmacks. Die entsprechenden Funktionen in summarytools sind dscr() bzw. stby() library(summarytools) descr(daten) ## Descriptive Statistics ## daten ## N: 2441 ## ## ctdi.vol dlp kv mas ## ----------------- ---------- --------- --------- --------- ## Mean 10.30 1126.55 114.47 156.93 ## Std.Dev 6.98 1142.93 18.77 92.65 ## Min 0.05 1.20 12.00 2.00 ## Q1 5.50 244.60 120.00 92.00 ## Median 8.00 576.00 120.00 150.00 ## Q3 14.30 1795.30 120.00 206.00 ## Max 39.60 5979.30 801.00 1231.00 ## MAD 5.93 622.10 0.00 85.99 ## IQR 8.80 1550.70 0.00 114.00 ## CV 0.68 1.01 0.16 0.59 ## Skewness 1.21 1.18 19.45 1.79 ## SE.Skewness 0.05 0.05 0.05 0.05 ## Kurtosis 1.40 0.51 731.66 13.34 ## N.Valid 2441.00 2441.00 2441.00 2441.00 ## Pct.Valid 100.00 100.00 100.00 100.00 # Da auch hier die Ausgabe sehr lang würde, wird sie hier nicht abgebildet. stby(daten, daten$untersuchung, descr) Noch viel praktischer, aber leider nicht nach Gruppen aufgetrennt durchführbar, ist die Funktion dfSummary(), insbesondere wenn man ihre Ausgabe weiterleitet an die Funktion view() aus dem summarytools Paket. Diese produziert dann eine ansprechend formatierte HTML-Datei, in der verschiedene Maßzahlen aufgeführt werden. Um nach Gruppen getrennte Ausgaben zu bekommen, kann man beispielsweise die tidyverse filter() Funktion benutzen, muss dann aber die Befehle für jede Gruppe erneut ausführen. # die Ausgaben dieser Funktionen erscheinen nicht auf der Konsole, # sondern im Viewer-Pane von RStudio. Dort findet man auch einen Button, # um die Datei in einem Internetbrowser zu öffnen. # einfacher Fall, um einen Überblick über alle Daten zu erhalten dfSummary(daten) %&gt;% view() # innerhalb einer tidyverse-Kette um bspw. einen Überblick über # die Traumaspiralen zu erhalten daten %&gt;% filter(untersuchung == &quot;Traumaspirale&quot;) %&gt;% dfSummary() %&gt;% view() 3.5 Mehr bunte Bilder Um deskriptive Statistiken auch grafisch nach Gruppen zu trennen, bietet sich die Funktion facet_wrap() an, die ebenfalls Teil von ggplot2 ist. Im Prinzip kann man sich die Funktionsweise ähnlich wie die by() Funktion vorstelen, wobei die Handhabung noch etwas simpler ist. Man definiert einfach wie gewohnt die Grafik die man mit ggplot() erstellen möchte, und fügt facet_wrap() einfach mit + in die Kette der Grafikfunktionen. Innerhalb von facet_wrap() muss noch etwas umständlich die entsprechende Gruppenvariable in die Funktion vars() eingeschlossen werden, aber die Ergebnisse sind sehr ansprechend. # ähnliche Grafik wie am Ende des letzten Kapitels, aber mit # nach Untersuchung getrennten Ausgaben, damit die Punkte # weniger überlappen. daten %&gt;% ggplot(aes(x = ctdi.vol, y = dlp, color = untersuchung)) + geom_point() + facet_wrap(vars(untersuchung)) + labs( x = &quot;CTDIvol in mGy&quot;, y = &quot;DLP in mGy*cm&quot;, title = &quot;Zusammenhang CTDIvol / DLP&quot;, color = &quot;Untersuchungsart&quot; ) + theme_bw() Natürlich funktioniert facet_wrap() nicht nur mit Punktwolken, sondern letztlich mit beliebigen Grafiken. Da sich für deskriptive Statistiken insbesondere Boxplots anbieten, im nachfolgenden Beispiel eine etwas komplexere Grafik. In dem Fall wird zunächst die Variable kv mithilfe der Funktion mutate() statt als numerische Variable als ordinale Variable definiert. Zur besseren Übersichtlichkeit werden nur die Fälle betrachtet mit 80kV, 100kV und 120kV. Der Rest ist dann wie gewohnt die Definition der Grafik und das Aufteilen mittels facet_wrap(). daten %&gt;% mutate(kv = as.factor(kv)) %&gt;% filter(kv %in% c(80, 100, 120)) %&gt;% ggplot(aes(x = kv, y = dlp, fill = kv)) + geom_boxplot() + facet_wrap(vars(untersuchung)) + labs( x = &quot;kV-Einstellung&quot;, y = &quot;DLP in mGy*cm&quot;, title = &quot;DLP in Abhängigkeit von kV-Einstellung&quot;, fill = &quot;kV-Einstellung&quot; ) + theme_bw() "],
["theoretisch-ja-praktisch-auch-tests-in-r.html", "4 Theoretisch ja, praktisch auch! Tests in R 4.1 Lernziele 4.2 Normalverteilung prüfen 4.3 T-Test und Wilcoxon-Test", " 4 Theoretisch ja, praktisch auch! Tests in R Üblicherweise geht es nach den deskriptiven Statistiken in den meisten Fällen ans Eingemachte - die statistischen Tests. Nehmen wir an, in zwei Gruppen beobachten wir unterschiedliche Mittelwerte. Eine mögliche Erklärung könnte sein, dass der bloße Zufall hier Unterschiede erscheinen lässt, wo in Wirklichkeit keine sind. Eine andere Möglichkeit wäre dann, dass in der Tat Unterschiede zwischen den Gruppen bestehen, und diese nicht alleine dem Zufall zu schulden sind. Die gesamten theoretischen Grundlagen hier zu wiederholen würde sicher den Rahmen sprengen, insofern beschränken wir uns im Folgenden mit der Durchführung verschiedener statistischer Tests in R. Welcher Test wann der geeignete ist, ist manchmal nicht leicht zu erkennen. Für eine erste grobe Einschätzung kann aber die folgende Tabelle hilfreich sein. Mögliche statistische Tests in Abhängigkeit der Variablen. (Tabelle ursprünglich aus Wikipedia unter Creative Commons Lizenz) 4.1 Lernziele Daten auf Normalverteilung prüfen (Shapiro-Test) Einfache statistische Tests rechnen (T-Test, Wilcoxon-Test) Effektstärke berechnen und eigene Funktionen schreiben ANOVA durchführen 4.2 Normalverteilung prüfen Wie aus obiger Tabelle ersichtlich ist, kann es mitunter zur Auswahl des korrekten statistischen Tests wichtig sein zunächst zu prüfen, ob die Werte einer Variablen normalverteilt sind. Natürlich können wir versuchen visuell abzuschätzen, ob dies der Fall ist. Für ein fiktives Beispiel wären wir zudem interessiert, die Normalverteilung für einen Wert innerhalb verschiedener Gruppen zu untersuchen. Eine schnelle deskriptive Statistik bzw. Visualisierung könnte wie folgt aussehen. Wir nutzen hier die skew()-Funktion aus dem psych Paket, um die Schiefe der Verteilung zu erhalten (bei einer Normalverteilung nahe 0, bei positiven Werten ist der Ausläufer nach rechts länger, bei negativen Werten der Ausläufer nach links). library(psych) daten %&gt;% group_by(untersuchung) %&gt;% summarise(schiefe_dlp = skew(dlp)) ## # A tibble: 6 x 2 ## untersuchung schiefe_dlp ## &lt;chr&gt; &lt;dbl&gt; ## 1 Abdomen 2.10 ## 2 Becken 3.39 ## 3 Gesicht 2.57 ## 4 Kopf 0.223 ## 5 Thorax 3.58 ## 6 Traumaspirale 0.0290 daten %&gt;% ggplot(aes(x = dlp, fill = untersuchung)) + geom_density() + facet_wrap(vars(untersuchung)) + labs(x = &quot;DLP [mGy*cm]&quot;, y = &quot;&quot;, fill = &quot;Untersuchungsbezeichnung&quot;) + theme_bw() Wir sehen hier bereits, dass die DLP-Werte für Untersuchungen des Kopfes und bei Traumaspiralen vermutlich normalverteilt sind. Doch um wirklich sicher zu gehen, sollte ein entsprechender statistischer Test verwendet werden. Für diese Frage bietet sich der Shapiro-Wilk-Test an. Der Shapiro-Wilk-Test nimmt als sog. Nullhypothese an, dass die Daten normalverteilt sind. Ein kleiner p-Wert zeigt in diesem Fall dann entsprechen an, dass die Alternativhypothese anzunehmen ist, also dass die Daten nicht normalverteilt sind. Erfreulicherweise findet sich für die meisten statistischen Tests in R eine entsprechend benannte Funktion. Um also bspw. die DLP-Werte im gesamten Datensatz auf Normalverteilung zu untersuchen, können wir einfach die Funktion shapiro.test() nutzen. shapiro.test(daten$dlp) ## ## Shapiro-Wilk normality test ## ## data: daten$dlp ## W = 0.8234, p-value &lt; 2.2e-16 In diesem Fall zeigt also der p-Wert &lt; 2,2 x 10-16 an, dass die Alternativhypothese angenommen werden sollte - die DLP-Werte im Gesamtdatensatz also nicht normalverteilt sind. 4.2.1 Ein kurzer Ausflug in Ergebnisobjekte In vielen Fällen sind die Ausgaben solcher statistischen Tests etwas unintuitiv formatiert und die einzelnen Werte weiterzuverwenden erscheint zunächst schwierig. Es lohnt daher an dieser Stelle ein kurzer Ausflug, denn wir können durchaus auf Einzelteile der Ausgabe direkt zugreifen. Um einen etwas tieferen Einblick zu erhalten, speichern wir zunächst das Ergebnis der shapiro.test() Funktion in einer Variablen und schauen uns die Struktur der Variablen an. shapiro.ergebnis &lt;- shapiro.test(daten$dlp) str(shapiro.ergebnis) ## List of 4 ## $ statistic: Named num 0.823 ## ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## $ p.value : num 8.48e-46 ## $ method : chr &quot;Shapiro-Wilk normality test&quot; ## $ data.name: chr &quot;daten$dlp&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; Wie man erkennen kann, handelt es sich um eine Liste von Werten, die einfach—wenn sie nicht in einer Variablen gespeichert sind—gemeinsam auf der Konsole ausgegeben werden. Das Dollarzeichen $ in der Ausgabe der str() Funktion zeigt hier aber bereits an, dass wir auch einzelne Teile des Ergebnisses direkt referenzieren können, ähnlich den Spalten eines Dataframes. shapiro.ergebnis$p.value ## [1] 8.48211e-46 shapiro.ergebnis$p.value &lt; 0.05 ## [1] TRUE Wären wir also bspw. in obigem fiktiven Beispiel daran interessiert die Normalverteilung innerhalb der Subgruppen zu prüfen, könnten wir in Tidyverse-Syntax eine Verkettung von Befehlen schreiben, an deren Ende eine ansprechend formatierte Tabelle stünde, die in einer Spalte auch gleich prüft, ob der p-Wert unterhalb des Signifikanzniveau von 0.05 liegt. daten %&gt;% group_by(untersuchung) %&gt;% summarise(shapiro_p.wert = shapiro.test(dlp)$p.value) %&gt;% mutate(signifikant_0.05 = shapiro_p.wert &lt; 0.05) ## # A tibble: 6 x 3 ## untersuchung shapiro_p.wert signifikant_0.05 ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Abdomen 1.48e-30 TRUE ## 2 Becken 4.46e-10 TRUE ## 3 Gesicht 1.01e-12 TRUE ## 4 Kopf 3.06e-20 TRUE ## 5 Thorax 4.29e-41 TRUE ## 6 Traumaspirale 5.19e- 5 TRUE Wir sehen also, dass—entgegen der ursprünglichen Annahme—auch in den Gruppen, die rein visuell und deskriptiv normalverteilt erschienen (Kopf und Traumaspirale), in Wirklichkeit keine Normalverteilung vorliegt. 4.3 T-Test und Wilcoxon-Test Nachdem nun die Daten auf Normalverteilung geprüft wurden, können wir mit dem entsprechenden statistischen Test untersuchen, ob zwischen einzelnen Gruppen ein signifikanter Unterschied in der Verteilung der Werte für eine Variable existiert. In unserem Falle wäre, da die Daten nicht normalverteilt sind, der Wilcoxon-Mann-Whitney-Test die richtige Wahl. Ähnlich wie für den Shapiro-Wilk-Test shapiro.test() existiert in diesem Falle die Funktion wilcoxon.test(). Die Funktion erwartet als Parameter entweder zwei Vektoren mit Zahlenwerten (oder ein Dataframe und eine sogenannte Formel, aber dazu später). Für ein einfaches Beispiel wollen wir den Vergleich der DLP-Werte von Thorax-Untersuchungen mit denen von Abdomen-Untersuchungen untersuchen. Hierzu ziehen wir uns zunächst die entsprechenden Werte aus dem Dataframe heraus und speichern sie in separaten Variablen, anschließend können diese Vektoren der Funktion wilcoxon.test() übergeben werden. Im Übrigen geht R bei T-Test und Wilcoxon-Test davon aus, dass es sich um unverbundene Stichproben handelt. Sicherheitshalber sollte man sich aber angewöhnen, dies auch explizit mithilfe des Parameters paired = FALSE anzugeben. # erst die Funktion pull() extrahiert aus dem Dataframe einen einfachen Vektor dlp.abdomen &lt;- daten %&gt;% filter(untersuchung == &quot;Abdomen&quot;) %&gt;% pull(dlp) dlp.thorax &lt;- daten %&gt;% filter(untersuchung == &quot;Thorax&quot;) %&gt;% pull(dlp) wilcox.test(dlp.abdomen, dlp.thorax, paired = FALSE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: dlp.abdomen and dlp.thorax ## W = 445818, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 Wie erwartet, zeigt hier der p-Wert &lt; 2,2 x 10-16 an, dass die DLP-Werte von Thorax-Untersuchungen und Abdomen-Untersuchungen signifikant verschieden sind. Genau so einfach ließe sich entsprechend auch ein T-Test t.test() rechnen, der zwar in diesem Falle vielleicht nicht optimal wäre, weil wie bereits besprochen keine Normalverteilung der Werte vorliegt, aber das ignorieren wir einmal. t.test(dlp.abdomen, dlp.thorax, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: dlp.abdomen and dlp.thorax ## t = 14.719, df = 1032.2, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 554.7071 725.3559 ## sample estimates: ## mean of x mean of y ## 1037.5276 397.4961 4.3.1 Ein kurzer Ausflug in Formeln Mit Formeln werden in R nicht übliche Formeln in der Art f(x) = mx + b bezeichnet, sondern bei der Definition von statistischen Tests oder Modellen ein Ausdruck, der beschreibt welche Zielvariable von welchen Einflussvariablen abhängen soll. Eine Formel in R besteht immer aus einer Tilde ~ und einem oder mehreren Ausdrücken, die jeweils rechts (RHS = right hand side) bzw. links (LHS = left hand side) der Tilde ~ stehen. Die Zielvariable steht dabei links (LHS) und die möglichen Einflussvariablen rechts (RHS). Wollten wir also beispielsweise ausdrücken, dass wir das DLP in Abhängigkeit der Untersuchungsbeschreibung auf statistische Signifikanz untersuchen wollen würden, könnten wir das folgendermaßen ausdrücken: dlp ~ untersuchung. Wollten wir also in unserem Beispiel die beiden Gruppen Thorax und Abdomen vergleichen, könnten wir den Ausdruck daten %&gt;% filter(untersuchung %in% c(\"Abdomen\", \"Thorax\")) nutzen, um ein Dataframe zu erzeugen, das nur diese beiden Gruppen enthält. Bei dem Aufruf der Funktion t.test() müssten wir dieses Dataframe als Parameter data = übergeben und als ersten Parameter die oben genannte Formel dlp ~ untersuchung. # auch hier hilft eine Schreibweise mit Zeilenumbrüchen nicht den Überblick zu verlieren t.test( dlp ~ untersuchung, data = daten %&gt;% filter(untersuchung %in% c(&quot;Abdomen&quot;, &quot;Thorax&quot;)), paired = FALSE ) ## ## Welch Two Sample t-test ## ## data: dlp by untersuchung ## t = 14.719, df = 1032.2, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 554.7071 725.3559 ## sample estimates: ## mean in group Abdomen mean in group Thorax ## 1037.5276 397.4961 Das mag zunächst etwas kompliziert wirken. Aber solche Schreibweisen haben den Vorteil, dass nicht für jeden Test neue Variablen definiert werden müssen, was bei langen und komplizierten Auswertungen schonmal dazu führen kann, dass man den Überblick verliert. Für einfache statistische Tests ist die Formelschreibweise nicht unbedingt nötig, und beide Herangehensweisen liefern identische Ergebnisse. Im späteren Kapitel zu Machine Learning werden wir allerdings um die Verwendung dieser Formelschreibweise nicht herum kommen. 4.3.2 Ein kurzer Ausflug in Funktionen Im Live-Webinar hatten wir darüber gesprochen, dass bspw. beim T-Test neben dem p-Wert auch die sog. Effektstärke relevant sein kann. Zwar existiert für R beispielsweise das Paket effsize, das Funktionen enthält um die Effektstärke zu berechnen, aber gelegentlich kommt vielleicht man an einen Punkt, wo auf die Schnelle kein Paket zu finden ist, dass genau die Funktion enthält, die man sucht. Für solche Fälle bietet R die Möglichkeit, eigene Funktionen zu definieren, die der Vollständigkeit hier genannt werden soll, obwohl diese spezielle Funktionalität für den Anfang vielleicht etwas komplex ist. Für ein kurzes Beispiel wollen wir einen Spezialfall betrachten, in dem für unverbundene Stichproben gleicher Gruppengröße die Effektstärke anhand von T-Statistik und df-Wert des T-Testes brechnet werden kann. Wir wollen dabei das Ergebnisobjekt der Funktion t.test() nehmen und direkt die Werte für T-Statistik und df extrahiert und sodann die Effektstärke berechnet werden kann. Eine Funktion kann einfach mithilfe der Funktion function() definiert werden, und in einer Variable gespeichert werden, die genutzt werden kann, um die Funktion aufzurufen. Innerhalb der runden Klammern () von function() können Parameter definiert werden, die dann innerhalb der Funktion verfügbar sind. Beim Aufruf kann auch an selbst definierte Formeln mithilfe des Verkettungsoperators %&gt;% ein Parameter übergeben werden. Die Möglichkeiten, die sich hierdurch ergeben sind schier endlos… meine_cohens_d_funktion &lt;- function(ergebnis_t.test) { # Die Funktion unname() ist hier für eine schönere Ausgabe am Ende # der Funktion nötig. Kann aber auch ignoriert werden. t_statistik &lt;- ergebnis_t.test$statistic %&gt;% unname() df_wert &lt;- ergebnis_t.test$parameter %&gt;% unname() # Formel für Cohens d # siehe auch https://www.uccs.edu/lbecker/ 2 * t_statistik / sqrt(df_wert) } t.test( dlp ~ untersuchung, data = daten %&gt;% filter(untersuchung %in% c(&quot;Abdomen&quot;, &quot;Thorax&quot;)), paired = FALSE ) %&gt;% meine_cohens_d_funktion() ## [1] 0.9162841 "]
]
