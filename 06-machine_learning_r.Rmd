# See one, do one, teach one! Machine Learning in R

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
```

Nachdem wir uns nun bereits in deskriptive Statistiken und statistische Tests eingearbeitet haben, können wir einen Schritt weiter gehen und kommen zu dem Thema, das aktuell mehr denn je in aller Munde ist - [Maschinelles Lernen bzw. Machine Learning](https://de.wikipedia.org/wiki/Maschinelles_Lernen). In seinen einfacheren Ausprägungen ist maschinelles Lernen nichts anderes als die Erarbeitung eines statistischen Modells, welches dann wiederum auf neue Daten angewandt werden kann, um bspw. Vorhersagen zu treffen.

![Künstliche Intelligenz, Maschinelles Lernen und Neuronale Netze](./assets/img/circles_of_ai.png)

Heute werden die Begriffe "Künstliche Intelligenz", "Maschinelles Lernen" und "Neuronale Netzwerke" teils synonym verwandt. Korrekterweise kann man jedoch sagen, dass die neuronalen Netze ledigliche ein Teilgebiet des maschinellen Lernens sind, das wiederum ein Teil von dem bezeichnet was unter künstlicher Intelligenz verstanden wird. Im folgenden werden wir einige einfachere Algorithmen kennenlernen und in R ausprobiern. Wer danach noch Hunger auf mehr Machine Learning Algorithmen und deren Anwendung in R hat, dem sei die Webseite [101 Machine Learning Algorithms](https://blog.datasciencedojo.com/machine-learning-algorithms/) ans Herz gelegt.

## Lernziele

1. Verschiedene Machine Learning Algorithmen in R nutzen
2. Daten in Training- und Testdaten unterteilen
3. Trainierte Modelle auf neue Daten anwenden
4. Klassifikationsgüte berechnen

## Das Iris Dataset

In vielen Beipsielen wird online auf das sogenannte [Iris Dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) Bezug genommen. Der Datensatz hat deshalb einige Berühmtheit erlangt, der Einfachheit halber verwenden wir ihn deshalb auch für dieses Webinar.

```{r}
data("iris")
str(iris)
```

In dem Datensatz enthalten sind Beobachtungen zu 150 verschiedenen Schwertlilien enthalten, jede Beobachtung enthält Angaben zu Länge und Breite der Kelch- und Kronblätter sowie zur Zugehörigkeit zu einer von drei Schwertlilienarten. Die übliche Aufgabe ist es dann anhand dieses Datensatzes einen Algorithmus zu trainieren, der aus den Angaben zur Länge und Breite der Kelch- und Kronblätter Vorhersagen über die Artzugehörigkeit trifft.

## Explorative Datenvisualisierung

Ein guter Anfang ist meist sich einen visuellen Überblick über die Daten zu verschaffen. Man könnte beispielsweise die Verteilung der Messwerte als gruppierte Punktwolken darstellen.

```{r}
iris %>% 
  pivot_longer(-Species, names_to = "variable") %>% 
  ggplot(aes(x = Species, y = value, color = Species)) +
    geom_jitter() +
    facet_wrap(vars(variable))
```

Wie man sieht, müsste es möglich sein anhand der Daten die Artzugehörigkeit abzuschätzen. Rein visuell könnte man vermuten, dass sich dafür insbesondere die Länge der Kronblätter (`Petal-Lenght`) eignen müsste.

## Lineare Regression

Einer der einfachsten Machine Learning Algorithmen, die [Lineare Regression](https://de.wikipedia.org/wiki/Lineare_Regression) eignet sich zwar nicht zur Vorhersage eines kategorialen Variable, wie sie die Artzugehörigkeit ist, sollte aber hier trotzdem nicht unerwähnt bleiben. Üblicherweise versucht eine lineare Regression einen kontinuierlichen Zahlenwert für eine abhängige Variable aus einer oder mehreren unabhängigen Variablen zu berechnen. In unserem Fall könnten wir beispielsweise versuchen die Breite der Kronblätter (`Petal.Width`) aus den übrigen Variablen abzuschätzen.

Hierzu nutzen wir die Funktion `lm()`. Erster Parameter dieser Funktion ist eine Formel, wie wir sie bereits im Kapitel zu den statistischen Tests benutzt haben (Abschnitt \@ref(formulas-in-r)). In Formeln kann der Punkt `.` genutzt werden, um alle Variablen (bzw. Spalten) außer der links der Tilde `~` angegebenen zu referenzieren. Der zweite Parameter der Funktion ist das Dataframe, das genutzt werden soll. Da es hier keinen Sinn machen würde die Art einzuschließen, nutzen wir innerhalb der `lm()`-Funktion ein `select()`, um die Variable `Species` auszuschließen.

```{r}
fit <- lm(formula = Petal.Width ~ ., data = iris %>% select(-Species))
summary(fit)
```

Die Ausgabe ist auf den ersten Blick nicht besonders eingängig, zeigt aber im Wesentlichen schon, dass signifikante Zusammenhänge zwischen Kronblattbreite und allen anderen Variablen existieren.

Natürlich können auch die Ergebnisobjekte von Machine Learning Algorithmen für die weitere Nutzung mit Tidyverse-Paketen aufbereitet werden (siehe Abschnitt \@ref(broom-return-objects)).

```{r}
library(broom)
tidy(fit)

tidy(fit, conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(p.value.chr = format.pval(p.value, digits = 1)) %>% 
  ggplot(aes(x = estimate, y = fct_reorder(term, estimate))) +
    geom_point() +
    geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0.15) +
    geom_vline(xintercept = 0, linetype = "dashed") +
    geom_text(aes(label = paste("p-value =", p.value.chr)), nudge_y = 0.2) +
    scale_x_continuous(limits = c(-0.8, 0.8)) +
    labs(y = "",
         x = "Estimate")
```


----





## Praktische Pakete für Machine Learning

Für die meisten Anwedungen und Algorithmen existieren mehrere Pakete, mit vergleichbarer Funktionalität. In diesem Webinar haben wir die folgenden benutzt.

- [`broom`](https://github.com/tidymodels/broom) enthält etliche praktische Funktionen, um mit Modellen zu interagieren
- [`caret`](http://caret.r-forge.r-project.org) bietet Funktionen zum Präprozessieren und Aufteilen von Daten
- [`e1071`](https://www.rdocumentation.org/packages/e1071/versions/1.7-3) nutzen wir für das Training einer Support Vector Machine
- [`randomForest`](https://www.rdocumentation.org/packages/randomForest/) nutzen wir für das Training eines Random Forest
- [`class`](https://www.rdocumentation.org/packages/class) nutzen wir für das Training eines k-Nearest Neighbour Classifiers
- [`pROC`](https://web.expasy.org/pROC/) ist ein Paket mit Funktionen um ROC-Analysen durchzuführen

```{r message=FALSE, warning=FALSE}
library(broom)
library(caret)
library(e1071)
library(randomForest)
library(class)
library(pROC)
```



## Logistische Regression

Im einfachsten Fall versucht eine [logistische Regression](https://de.wikipedia.org/wiki/Logistische_Regression) eine binäre Entscheidung (abhängige Variable) aus einer oder mehreren unabhängigen Variablen zu treffen.

Wir werden für dieses und die weiteren Beispiele versuchen die Spezies der Pflanze aus ihren Messwerten zu Blattlängen und -breiten vorherzusagen.

```{r}
# Überschreibt die Variable my.formula von oben
my.formula <- Species ~ .
```

```{r}
# Erstellen einer Teilmenge des Datensatzes mit nur zwei Spezies
iris_small <- iris %>%
  filter(Species %in% c("virginica", "versicolor")) %>%
  # da die Variable Species weiterhin als Factor mit drei Level
  # angelegt wäre, nutzen wir die droplevels()-Funktion, um
  # Fehler zu vermeiden.
  droplevels()

# family = "binomial" gibt hier an, dass nur eine binäre Entscheidung zu treffen ist
fit <- glm(my.formula, data = iris_small, family = "binomial")
```

Das in der Variablen `fit` gespeicherte Modell, können wir nun nutzen um die Vorhersagewerte des Modells zu berechnen. Der besseren Übersicht halber speichern wir diese in einer neuen Spalte des ursprünglichen Dataframes.

```{r}
# wir nutzen hier die add_column()-Funktion aus dem tidyverse
# und die predict()-Funktion um Werte mithilfe eines Modells
# zu berechnen
iris_small_augmented <- iris_small %>% 
  add_column(vorhersage = predict(fit))

# ein Blick in die ersten Zeilen der Daten
head(iris_small_augmented)

# eine einfache Visualisierung
iris_small_augmented %>% 
  ggplot(aes(x = vorhersage, fill = Species)) +
    geom_density(alpha = 0.5)
```

Wir sehen der Vorhersagewert des Modells hat Potential zwischen den beiden Gruppen zu unterscheiden. Wann immer ein kontinuierlicher Vorhersagewert vorliegt, können wir eine ROC-Analyse rechnen.

## ROC-Analyse

```{r message=FALSE, warning=FALSE}
roc(iris_small_augmented$Species, iris_small_augmented$vorhersage)

roc(iris_small_augmented$Species, iris_small_augmented$vorhersage) %>% 
  ggroc(color = "steelblue", size = 1) +
    geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
                 color="grey", linetype="dashed") +
    theme_minimal() +
    coord_fixed()
```

